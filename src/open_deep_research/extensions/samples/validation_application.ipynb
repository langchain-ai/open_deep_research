{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep agentValidation Framework\n",
    "\n",
    "This notebook provides a comprehensive framework for evaluating deep agent.\n",
    "\n",
    "\n",
    "\n",
    "**Approach:** Sequential testing (one system at a time), no ground truth required.\n",
    "\n",
    "**Evaluation Methods:**\n",
    "1. LLM-as-Judge (automated quality scoring)\n",
    "2. Citation Verification (URL liveness + content support)\n",
    "3. DeepEval Metrics (faithfulness, relevancy)\n",
    "4. Manual Verification (freshness/recency checks)\n",
    "5. Cross-System Comparison (after both systems are tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install dependencies\n",
    "# Uncomment and run if not already installed\n",
    "\n",
    "# !pip install deepeval ragas openai anthropic requests beautifulsoup4 plotly pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports and environment setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Add project root to path so we can import EDR modules\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=PROJECT_ROOT / \".env\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Python path configured\")\n",
    "print(f\"Environment loaded from: {PROJECT_ROOT / '.env'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "\n",
    "CONFIG = {\n",
    "    # Which system are you testing right now?\n",
    "    # Options: \"sf_edr\" or \"open_deep_research\"\n",
    "    \"system_under_test\": \"sf_edr\",\n",
    "    \n",
    "    # sf EDR settings\n",
    "    \"edr_provider\": os.environ.get(\"LLM_PROVIDER\", \"google\"),\n",
    "    \"edr_model\": os.environ.get(\"LLM_MODEL\", \"gemini-2.5-pro\"),\n",
    "    \"edr_max_loops\": 3,\n",
    "    \n",
    "    # LLM Judge settings (use a different LLM than the system being tested)\n",
    "    \"judge_provider\": \"openai\",  # or \"anthropic\"\n",
    "    \"judge_model\": \"gpt-4o\",     # or \"claude-sonnet-4-5-20250929\"\n",
    "    \n",
    "    # Output settings\n",
    "    \"output_dir\": str(PROJECT_ROOT / \"validation_results\"),\n",
    "    \"run_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Test Query Bank\n",
    "\n",
    "All test queries organized by category. Each query has:\n",
    "- `id`: Unique identifier\n",
    "- `query`: The research question\n",
    "- `category`: High-level category (factual, stress, manual_verification, etc.)\n",
    "- `subcategory`: Specific test type within the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define all test queries\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    # === A. Factual / Verifiable Queries ===\n",
    "    {\"id\": \"A1\", \"query\": \"What were the top 5 causes of the 2008 financial crisis, with supporting data?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A2\", \"query\": \"List all Nobel Prize winners in Physics from 2020-2024 and their contributions.\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A3\", \"query\": \"What is the current market share of cloud providers (AWS, Azure, GCP) as of 2024?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A4\", \"query\": \"Summarize the key provisions of the EU AI Act.\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A5\", \"query\": \"What are the FDA-approved treatments for Type 2 diabetes as of 2024?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    \n",
    "    # === B. Multi-Hop Reasoning Queries ===\n",
    "    {\"id\": \"B1\", \"query\": \"How did the semiconductor chip shortage of 2020-2023 affect both the automotive and gaming industries differently?\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    {\"id\": \"B2\", \"query\": \"Compare the economic policies of the US and EU regarding AI regulation and their downstream effects on startup funding.\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    {\"id\": \"B3\", \"query\": \"Trace the supply chain of lithium from mining to EV batteries - what are the geopolitical risks at each stage?\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    \n",
    "    # === C. Ambiguous / Open-Ended Queries ===\n",
    "    {\"id\": \"C1\", \"query\": \"What's the best programming language?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    {\"id\": \"C2\", \"query\": \"Is AI dangerous?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    {\"id\": \"C3\", \"query\": \"Tell me about the Apple situation\", \"category\": \"ambiguous\", \"subcategory\": \"ambiguous_entity\"},\n",
    "    {\"id\": \"C4\", \"query\": \"What happened recently in tech?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    \n",
    "    # === D. Stress Tests ===\n",
    "    # D1: Contradictory sources\n",
    "    {\"id\": \"D1a\", \"query\": \"Is coffee good or bad for health? Provide evidence for both sides.\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    {\"id\": \"D1b\", \"query\": \"Is remote work more productive than in-office? What does the research say?\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    {\"id\": \"D1c\", \"query\": \"Are electric vehicles truly better for the environment when considering full lifecycle?\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    \n",
    "    # D2: Obscure topics\n",
    "    {\"id\": \"D2a\", \"query\": \"What is the history of the Voynich Manuscript's ownership chain?\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2b\", \"query\": \"Describe the political structure of the Principality of Sealand.\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2c\", \"query\": \"What are the known side effects of the drug Zuranolone in postpartum depression?\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2d\", \"query\": \"Summarize the contributions of Srinivasa Ramanujan's lost notebook to number theory.\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    \n",
    "    # D3: Very recent events\n",
    "    {\"id\": \"D3a\", \"query\": \"What were the most significant tech industry events in the past 48 hours?\", \"category\": \"stress\", \"subcategory\": \"freshness\"},\n",
    "    {\"id\": \"D3b\", \"query\": \"What is the latest stock price movement for NVIDIA and why?\", \"category\": \"stress\", \"subcategory\": \"freshness\"},\n",
    "    \n",
    "    # D4: Highly technical\n",
    "    {\"id\": \"D4a\", \"query\": \"Explain the differences between LoRA, QLoRA, and DoRA fine-tuning methods with benchmark comparisons.\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    {\"id\": \"D4b\", \"query\": \"Compare the architectures of Mamba, RWKV, and Transformer models for sequence modeling.\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    {\"id\": \"D4c\", \"query\": \"What is the current state of topological quantum computing at Microsoft and IBM?\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    \n",
    "    # D5: Long-form output\n",
    "    {\"id\": \"D5a\", \"query\": \"Write a comprehensive 5000-word research report on quantum computing's impact on cryptography.\", \"category\": \"stress\", \"subcategory\": \"long_form\"},\n",
    "    \n",
    "    # D7: Edge cases\n",
    "    {\"id\": \"D7a\", \"query\": \"research\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    {\"id\": \"D7b\", \"query\": \"asdfghjkl qwerty zxcvbnm research this\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    {\"id\": \"D7c\", \"query\": \"Write a short but comprehensive 10,000-word summary\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    \n",
    "    # D9: Multi-language\n",
    "    {\"id\": \"D9a\", \"query\": \"Summarize the key findings of recent Chinese AI research papers on large language models.\", \"category\": \"stress\", \"subcategory\": \"multi_language\"},\n",
    "    {\"id\": \"D9b\", \"query\": \"What are the latest German automotive industry reports on EV adoption?\", \"category\": \"stress\", \"subcategory\": \"multi_language\"},\n",
    "    \n",
    "    # D10: Numerical accuracy\n",
    "    {\"id\": \"D10a\", \"query\": \"What were the exact GDP growth rates for G7 countries in 2024 Q1-Q4?\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    {\"id\": \"D10b\", \"query\": \"List the top 10 most funded AI startups in 2024 with their exact funding amounts.\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    {\"id\": \"D10c\", \"query\": \"What are the current interest rates set by the Fed, ECB, and Bank of Japan?\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    \n",
    "    # === E. Domain-Specific Queries ===\n",
    "    {\"id\": \"E1\", \"query\": \"What are the key differences between GDPR and CCPA?\", \"category\": \"domain_specific\", \"subcategory\": \"legal\"},\n",
    "    {\"id\": \"E2\", \"query\": \"What is the current evidence on intermittent fasting for cardiovascular health?\", \"category\": \"domain_specific\", \"subcategory\": \"medical\"},\n",
    "    {\"id\": \"E3\", \"query\": \"Analyze Tesla's Q3 2024 earnings - what are the key takeaways?\", \"category\": \"domain_specific\", \"subcategory\": \"financial\"},\n",
    "    {\"id\": \"E4\", \"query\": \"What is the current state of nuclear fusion research?\", \"category\": \"domain_specific\", \"subcategory\": \"scientific\"},\n",
    "    \n",
    "    # === F. Recent Research Generation ===\n",
    "    {\"id\": \"F1\", \"query\": \"Find and summarize the 5 most recent research papers on LLM hallucination mitigation published in 2025-2026.\", \"category\": \"recent_research\", \"subcategory\": \"papers\"},\n",
    "    {\"id\": \"F2\", \"query\": \"What are the latest breakthroughs in solid-state batteries from the past 6 months?\", \"category\": \"recent_research\", \"subcategory\": \"breakthroughs\"},\n",
    "    {\"id\": \"F3\", \"query\": \"Summarize recent clinical trial results for GLP-1 receptor agonists in 2025.\", \"category\": \"recent_research\", \"subcategory\": \"clinical_trials\"},\n",
    "    \n",
    "    # === G. Manual Verification Queries (Freshness Check) ===\n",
    "    # G1: Live/real-time data\n",
    "    {\"id\": \"G1a\", \"query\": \"What is today's price of Bitcoin?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    {\"id\": \"G1b\", \"query\": \"What is the current USD to INR exchange rate?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    {\"id\": \"G1c\", \"query\": \"What is NVIDIA's stock price right now?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    \n",
    "    # G2: Recent events\n",
    "    {\"id\": \"G2a\", \"query\": \"What were the top tech news stories this week?\", \"category\": \"manual_verification\", \"subcategory\": \"recent_events\"},\n",
    "    \n",
    "    # G3: Recently changed facts\n",
    "    {\"id\": \"G3a\", \"query\": \"What is the latest version of Python?\", \"category\": \"manual_verification\", \"subcategory\": \"changed_facts\"},\n",
    "    {\"id\": \"G3b\", \"query\": \"What is the current US federal interest rate?\", \"category\": \"manual_verification\", \"subcategory\": \"changed_facts\"},\n",
    "    \n",
    "    # G5: Trick questions\n",
    "    {\"id\": \"G5a\", \"query\": \"What is the latest iPhone model?\", \"category\": \"manual_verification\", \"subcategory\": \"trick\"},\n",
    "    {\"id\": \"G5b\", \"query\": \"What is the most recent SpaceX Starship launch result?\", \"category\": \"manual_verification\", \"subcategory\": \"trick\"},\n",
    "]\n",
    "\n",
    "# Summary\n",
    "df_queries = pd.DataFrame(TEST_QUERIES)\n",
    "print(f\"Total test queries: {len(TEST_QUERIES)}\")\n",
    "print(f\"\\nQueries by category:\")\n",
    "print(df_queries['category'].value_counts().to_string())\n",
    "print(f\"\\nQueries by subcategory:\")\n",
    "print(df_queries['subcategory'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Research Runner (Adapter Pattern)\n",
    "\n",
    "One standardized interface, multiple backends. Each adapter returns:\n",
    "```python\n",
    "{\n",
    "    \"query\": str,\n",
    "    \"response_text\": str,\n",
    "    \"sources\": List[str],\n",
    "    \"timing_seconds\": float,\n",
    "    \"word_count\": int,\n",
    "    \"metadata\": dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: SF enterprise EDR Adapter\n",
    "\n",
    "async def run_sf_edr(query: str, config: dict) -> dict:\n",
    "    \"\"\"Run a query through SF Enterprise Deep Research.\n",
    "    Wraps the existing run_research_sync() from benchmarks/run_research.py.\n",
    "    \"\"\"\n",
    "    from benchmarks.run_research import run_research_sync\n",
    "    \n",
    "    output_file = os.path.join(\n",
    "        config[\"output_dir\"],\n",
    "        f\"edr_raw_{config['run_id']}_{query[:30].replace(' ', '_')}.json\"\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = await run_research_sync(\n",
    "            query=query,\n",
    "            max_web_search_loops=config.get(\"edr_max_loops\", 3),\n",
    "            visualization_disabled=True,\n",
    "            provider=config.get(\"edr_provider\"),\n",
    "            model=config.get(\"edr_model\"),\n",
    "            output_file=output_file,\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result is None:\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response_text\": \"\",\n",
    "                \"sources\": [],\n",
    "                \"timing_seconds\": elapsed,\n",
    "                \"word_count\": 0,\n",
    "                \"error\": \"run_research_sync returned None\",\n",
    "                \"metadata\": {\"system\": \"sf_edr\", \"provider\": config.get(\"edr_provider\"), \"model\": config.get(\"edr_model\")}\n",
    "            }\n",
    "        \n",
    "        response_text = result.get(\"article\", result.get(\"summary\", \"\"))\n",
    "        sources = result.get(\"debug_info\", {}).get(\"sources_gathered\", [])\n",
    "        if isinstance(sources, int):\n",
    "            sources = []  # sources_gathered is sometimes a count\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": response_text,\n",
    "            \"sources\": sources if isinstance(sources, list) else [],\n",
    "            \"timing_seconds\": result.get(\"timing\", {}).get(\"total_duration_seconds\", elapsed),\n",
    "            \"word_count\": len(response_text.split()),\n",
    "            \"metadata\": {\n",
    "                \"system\": \"sf_edr\",\n",
    "                \"provider\": config.get(\"edr_provider\"),\n",
    "                \"model\": config.get(\"edr_model\"),\n",
    "                \"research_loops\": result.get(\"debug_info\", {}).get(\"research_loops\", 0),\n",
    "                \"sources_count\": result.get(\"debug_info\", {}).get(\"sources_gathered\", 0),\n",
    "                \"raw_output_file\": output_file,\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": elapsed,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"metadata\": {\"system\": \"sf_edr\", \"provider\": config.get(\"edr_provider\"), \"model\": config.get(\"edr_model\")}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Open Deep agentAdapter (Placeholder)\n",
    "\n",
    "async def run_open_deep_research(query: str, config: dict) -> dict:\n",
    "    \"\"\"Run a query through LangChain Open Deep Research.\n",
    "    \n",
    "    TODO: Replace this placeholder with your actual Open Deep agentAPI call.\n",
    "    The function should return the same standardized dict format.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # ============================================================\n",
    "        # TODO: Replace this block with your Open Deep agentcall\n",
    "        # Example:\n",
    "        #   from open_deep_research import research\n",
    "        #   result = await research(query)\n",
    "        #   response_text = result.report\n",
    "        #   sources = result.sources\n",
    "        # ============================================================\n",
    "        \n",
    "        raise NotImplementedError(\n",
    "            \"Open Deep agentadapter not yet configured. \"\n",
    "            \"Please implement the API call in this cell.\"\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": response_text,\n",
    "            \"sources\": sources,\n",
    "            \"timing_seconds\": elapsed,\n",
    "            \"word_count\": len(response_text.split()),\n",
    "            \"metadata\": {\"system\": \"open_deep_research\"}\n",
    "        }\n",
    "    except NotImplementedError:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": elapsed,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"metadata\": {\"system\": \"open_deep_research\"}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Dispatcher\n",
    "\n",
    "async def run_research(query: str, system: str, config: dict) -> dict:\n",
    "    \"\"\"Route a research query to the appropriate system adapter.\"\"\"\n",
    "    adapters = {\n",
    "        \"sf_edr\": run_sf_edr,\n",
    "        \"open_deep_research\": run_open_deep_research,\n",
    "    }\n",
    "    \n",
    "    adapter = adapters.get(system)\n",
    "    if adapter is None:\n",
    "        raise ValueError(f\"Unknown system: {system}. Choose from: {list(adapters.keys())}\")\n",
    "    \n",
    "    return await adapter(query, config)\n",
    "\n",
    "print(f\"Dispatcher ready. System under test: {CONFIG['system_under_test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Execute Research Queries\n",
    "\n",
    "Run all test queries through the selected system. Results are saved incrementally to JSON after each query, so no data is lost if the process is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Run all queries\n",
    "\n",
    "RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"results_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "# Load existing results if resuming a partial run\n",
    "if os.path.exists(RESULTS_FILE):\n",
    "    with open(RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "    completed_ids = {r[\"query_id\"] for r in results}\n",
    "    print(f\"Resuming: {len(completed_ids)} queries already completed\")\n",
    "else:\n",
    "    results = []\n",
    "    completed_ids = set()\n",
    "\n",
    "# Filter to queries not yet completed\n",
    "pending_queries = [q for q in TEST_QUERIES if q[\"id\"] not in completed_ids]\n",
    "print(f\"Queries to run: {len(pending_queries)} / {len(TEST_QUERIES)}\")\n",
    "\n",
    "for i, query_info in enumerate(pending_queries):\n",
    "    print(f\"\\n[{i+1}/{len(pending_queries)}] Running: {query_info['id']} - {query_info['query'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = await run_research(\n",
    "            query=query_info[\"query\"],\n",
    "            system=CONFIG[\"system_under_test\"],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        \n",
    "        # Attach query metadata\n",
    "        result[\"query_id\"] = query_info[\"id\"]\n",
    "        result[\"category\"] = query_info[\"category\"]\n",
    "        result[\"subcategory\"] = query_info[\"subcategory\"]\n",
    "        result[\"system\"] = CONFIG[\"system_under_test\"]\n",
    "        result[\"timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Incremental save\n",
    "        with open(RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        status = \"ERROR\" if result.get(\"error\") else \"OK\"\n",
    "        print(f\"  [{status}] {result['word_count']} words, {result['timing_seconds']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [FAILED] {e}\")\n",
    "        results.append({\n",
    "            \"query_id\": query_info[\"id\"],\n",
    "            \"query\": query_info[\"query\"],\n",
    "            \"category\": query_info[\"category\"],\n",
    "            \"subcategory\": query_info[\"subcategory\"],\n",
    "            \"system\": CONFIG[\"system_under_test\"],\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": 0,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        })\n",
    "        with open(RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nAll queries complete. Results saved to: {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Quick summary of results\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "total = len(df_results)\n",
    "success = len(df_results[~df_results.get(\"error\", pd.Series(dtype=str)).notna() | (df_results.get(\"error\", pd.Series(dtype=str)) == \"\")])\n",
    "errors = total - success\n",
    "\n",
    "print(f\"=== Execution Summary ===\")\n",
    "print(f\"Total queries run:   {total}\")\n",
    "print(f\"Successful:          {total - len(df_results[df_results.get('error', '').astype(bool)])}\")\n",
    "print(f\"Average latency:     {df_results['timing_seconds'].mean():.1f}s\")\n",
    "print(f\"Median latency:      {df_results['timing_seconds'].median():.1f}s\")\n",
    "print(f\"Average word count:  {df_results['word_count'].mean():.0f}\")\n",
    "print(f\"\\nLatency by category:\")\n",
    "print(df_results.groupby('category')['timing_seconds'].agg(['mean', 'median', 'max']).round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Automated Evaluation - LLM-as-Judge\n",
    "\n",
    "Uses a strong LLM (GPT-4 / Claude) to automatically score each research output on multiple dimensions. **No ground truth required** - the judge evaluates standalone quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: LLM-as-Judge scoring function\n",
    "\n",
    "def get_judge_client(config: dict):\n",
    "    \"\"\"Initialize the judge LLM client.\"\"\"\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    if provider == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        return OpenAI(), config.get(\"judge_model\", \"gpt-4o\")\n",
    "    elif provider == \"anthropic\":\n",
    "        from anthropic import Anthropic\n",
    "        return Anthropic(), config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported judge provider: {provider}\")\n",
    "\n",
    "\n",
    "def llm_judge_score(query: str, response: str, config: dict) -> dict:\n",
    "    \"\"\"Score a research response using an LLM judge. No ground truth needed.\"\"\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"You are an expert research quality evaluator. Score the following research output.\n",
    "There is NO ground truth - evaluate the response on its own merits.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "RESPONSE:\n",
    "{response[:8000]}\n",
    "\n",
    "Score each dimension on a 1-5 scale (5 is best):\n",
    "\n",
    "1. relevancy (1-5): Does the response directly address the query?\n",
    "2. depth (1-5): How thorough and comprehensive is the coverage?\n",
    "3. source_quality (1-5): Are citations from reputable, relevant sources? Are sources properly referenced?\n",
    "4. coherence (1-5): Is the response well-structured, logical, and readable?\n",
    "5. confidence_calibration (1-5): Does it appropriately express uncertainty where warranted? (5 = good calibration)\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no explanation outside the JSON):\n",
    "{{\"relevancy\": X, \"depth\": X, \"source_quality\": X, \"coherence\": X, \"confidence_calibration\": X, \"reasoning\": \"brief 1-2 sentence explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    \n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported judge provider: {provider}\"}\n",
    "        \n",
    "        # Parse JSON from response (handle markdown code blocks)\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        \n",
    "        return json.loads(raw)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(\"LLM-as-Judge function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Run LLM-as-Judge on all results\n",
    "\n",
    "JUDGE_RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"judge_scores_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "# Load existing judge scores if resuming\n",
    "if os.path.exists(JUDGE_RESULTS_FILE):\n",
    "    with open(JUDGE_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        judge_scores = json.load(f)\n",
    "    judged_ids = {s[\"query_id\"] for s in judge_scores}\n",
    "    print(f\"Resuming: {len(judged_ids)} already judged\")\n",
    "else:\n",
    "    judge_scores = []\n",
    "    judged_ids = set()\n",
    "\n",
    "pending = [r for r in results if r[\"query_id\"] not in judged_ids and r.get(\"response_text\")]\n",
    "print(f\"Queries to judge: {len(pending)}\")\n",
    "\n",
    "for i, result in enumerate(pending):\n",
    "    print(f\"  [{i+1}/{len(pending)}] Judging {result['query_id']}...\", end=\" \")\n",
    "    \n",
    "    score = llm_judge_score(result[\"query\"], result[\"response_text\"], CONFIG)\n",
    "    score[\"query_id\"] = result[\"query_id\"]\n",
    "    score[\"category\"] = result[\"category\"]\n",
    "    score[\"subcategory\"] = result[\"subcategory\"]\n",
    "    \n",
    "    judge_scores.append(score)\n",
    "    \n",
    "    # Incremental save\n",
    "    with open(JUDGE_RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(judge_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    if \"error\" in score:\n",
    "        print(f\"ERROR: {score['error']}\")\n",
    "    else:\n",
    "        avg = (score[\"relevancy\"] + score[\"depth\"] + score[\"source_quality\"] + score[\"coherence\"] + score[\"confidence_calibration\"]) / 5\n",
    "        print(f\"avg={avg:.1f}\")\n",
    "\n",
    "print(f\"\\nJudge scores saved to: {JUDGE_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Display judge scores\n",
    "\n",
    "valid_scores = [s for s in judge_scores if \"error\" not in s]\n",
    "df_scores = pd.DataFrame(valid_scores)\n",
    "\n",
    "score_cols = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]\n",
    "\n",
    "if not df_scores.empty:\n",
    "    # Overall averages\n",
    "    print(\"=== Overall LLM-as-Judge Scores ===\")\n",
    "    print(df_scores[score_cols].mean().round(2).to_string())\n",
    "    \n",
    "    # By category\n",
    "    print(\"\\n=== Scores by Category ===\")\n",
    "    display(df_scores.groupby(\"category\")[score_cols].mean().round(2))\n",
    "    \n",
    "    # Full table\n",
    "    print(\"\\n=== All Scores ===\")\n",
    "    display(df_scores[[\"query_id\", \"category\"] + score_cols + [\"reasoning\"]].sort_values(\"category\"))\n",
    "else:\n",
    "    print(\"No valid judge scores yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Automated Evaluation - Citation Verification\n",
    "\n",
    "Checks whether cited URLs are alive and whether the source content actually supports the claims made in the research output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Extract URLs from response text\n",
    "\n",
    "def extract_urls(text: str) -> list:\n",
    "    \"\"\"Extract all URLs from text.\"\"\"\n",
    "    url_pattern = r'https?://[^\\s\\)\\]\\\"\\'>]+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    # Clean trailing punctuation\n",
    "    cleaned = []\n",
    "    for url in urls:\n",
    "        url = url.rstrip('.,;:!?')\n",
    "        if url not in cleaned:\n",
    "            cleaned.append(url)\n",
    "    return cleaned\n",
    "\n",
    "# Quick test\n",
    "test_text = \"See https://example.com/article and also https://test.org/paper.pdf for details.\"\n",
    "print(f\"Test URL extraction: {extract_urls(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Check if URLs are alive\n",
    "\n",
    "def check_url_alive(url: str, timeout: int = 10) -> dict:\n",
    "    \"\"\"Check if a URL responds with HTTP 200.\"\"\"\n",
    "    try:\n",
    "        r = requests.head(url, timeout=timeout, allow_redirects=True,\n",
    "                         headers={\"User-Agent\": \"Mozilla/5.0 (research-validator)\"})\n",
    "        return {\"url\": url, \"alive\": r.status_code < 400, \"status_code\": r.status_code}\n",
    "    except requests.exceptions.Timeout:\n",
    "        return {\"url\": url, \"alive\": False, \"status_code\": \"timeout\"}\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"alive\": False, \"status_code\": str(e)}\n",
    "\n",
    "print(\"URL liveness checker ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: LLM-based citation content verification\n",
    "\n",
    "def fetch_page_text(url: str, max_chars: int = 3000) -> Optional[str]:\n",
    "    \"\"\"Fetch and extract text content from a URL.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15, headers={\"User-Agent\": \"Mozilla/5.0 (research-validator)\"})\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # Remove script/style elements\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        return text[:max_chars]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def verify_citation_with_llm(response_text: str, url: str, source_content: str, config: dict) -> dict:\n",
    "    \"\"\"Use LLM to check if the source content supports claims near the URL in the response.\"\"\"\n",
    "    \n",
    "    # Extract text near the URL reference\n",
    "    url_pos = response_text.find(url)\n",
    "    if url_pos == -1:\n",
    "        # Try partial URL match\n",
    "        for part in url.split(\"/\")[2:4]:\n",
    "            if part in response_text:\n",
    "                url_pos = response_text.find(part)\n",
    "                break\n",
    "    \n",
    "    context_start = max(0, url_pos - 500) if url_pos >= 0 else 0\n",
    "    context_end = min(len(response_text), url_pos + 500) if url_pos >= 0 else 1000\n",
    "    claim_context = response_text[context_start:context_end]\n",
    "    \n",
    "    prompt = f\"\"\"Does the source content support the claims made in the research text near this citation?\n",
    "\n",
    "RESEARCH TEXT (near citation):\n",
    "{claim_context}\n",
    "\n",
    "SOURCE CONTENT:\n",
    "{source_content[:2000]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"verdict\": \"supported\" or \"partially_supported\" or \"unsupported\" or \"unrelated\", \"confidence\": 0.0 to 1.0, \"reasoning\": \"brief explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=300,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"verdict\": \"error\", \"confidence\": 0, \"reasoning\": f\"Unsupported provider: {provider}\"}\n",
    "        \n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        return {\"verdict\": \"error\", \"confidence\": 0, \"reasoning\": str(e)}\n",
    "\n",
    "print(\"Citation verification functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Run full citation verification pipeline\n",
    "\n",
    "CITATION_RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"citation_verification_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "citation_results = []\n",
    "\n",
    "for result in results:\n",
    "    if not result.get(\"response_text\"):\n",
    "        continue\n",
    "    \n",
    "    urls = extract_urls(result[\"response_text\"])\n",
    "    if not urls:\n",
    "        citation_results.append({\n",
    "            \"query_id\": result[\"query_id\"],\n",
    "            \"total_citations\": 0,\n",
    "            \"alive\": 0,\n",
    "            \"dead\": 0,\n",
    "            \"supported\": 0,\n",
    "            \"unsupported\": 0,\n",
    "            \"details\": [],\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    print(f\"Verifying {len(urls)} citations for {result['query_id']}...\", end=\" \")\n",
    "    \n",
    "    query_citations = []\n",
    "    for url in urls[:10]:  # Limit to 10 URLs per query to save time/cost\n",
    "        alive_check = check_url_alive(url)\n",
    "        \n",
    "        verification = {\"verdict\": \"not_checked\"}\n",
    "        if alive_check[\"alive\"]:\n",
    "            page_text = fetch_page_text(url)\n",
    "            if page_text:\n",
    "                verification = verify_citation_with_llm(\n",
    "                    result[\"response_text\"], url, page_text, CONFIG\n",
    "                )\n",
    "        \n",
    "        query_citations.append({\n",
    "            \"url\": url,\n",
    "            \"alive\": alive_check[\"alive\"],\n",
    "            \"status_code\": alive_check[\"status_code\"],\n",
    "            \"verdict\": verification.get(\"verdict\", \"not_checked\"),\n",
    "            \"confidence\": verification.get(\"confidence\", 0),\n",
    "        })\n",
    "    \n",
    "    alive_count = sum(1 for c in query_citations if c[\"alive\"])\n",
    "    supported_count = sum(1 for c in query_citations if c[\"verdict\"] in [\"supported\", \"partially_supported\"])\n",
    "    \n",
    "    citation_results.append({\n",
    "        \"query_id\": result[\"query_id\"],\n",
    "        \"total_citations\": len(query_citations),\n",
    "        \"alive\": alive_count,\n",
    "        \"dead\": len(query_citations) - alive_count,\n",
    "        \"supported\": supported_count,\n",
    "        \"unsupported\": sum(1 for c in query_citations if c[\"verdict\"] == \"unsupported\"),\n",
    "        \"details\": query_citations,\n",
    "    })\n",
    "    \n",
    "    print(f\"alive={alive_count}/{len(query_citations)}, supported={supported_count}\")\n",
    "\n",
    "# Save\n",
    "with open(CITATION_RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(citation_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nCitation results saved to: {CITATION_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Display citation verification results\n",
    "\n",
    "df_citations = pd.DataFrame([{\n",
    "    \"query_id\": c[\"query_id\"],\n",
    "    \"total_urls\": c[\"total_citations\"],\n",
    "    \"alive\": c[\"alive\"],\n",
    "    \"dead\": c[\"dead\"],\n",
    "    \"supported\": c[\"supported\"],\n",
    "    \"unsupported\": c[\"unsupported\"],\n",
    "    \"alive_rate\": c[\"alive\"] / c[\"total_citations\"] if c[\"total_citations\"] > 0 else 0,\n",
    "    \"support_rate\": c[\"supported\"] / c[\"alive\"] if c[\"alive\"] > 0 else 0,\n",
    "} for c in citation_results])\n",
    "\n",
    "if not df_citations.empty:\n",
    "    print(\"=== Citation Verification Summary ===\")\n",
    "    with_citations = df_citations[df_citations[\"total_urls\"] > 0]\n",
    "    \n",
    "    if not with_citations.empty:\n",
    "        print(f\"Queries with citations: {len(with_citations)} / {len(df_citations)}\")\n",
    "        print(f\"Average URLs per query: {with_citations['total_urls'].mean():.1f}\")\n",
    "        print(f\"Average alive rate:     {with_citations['alive_rate'].mean():.1%}\")\n",
    "        print(f\"Average support rate:   {with_citations['support_rate'].mean():.1%}\")\n",
    "        print(f\"\\nPer-query breakdown:\")\n",
    "        display(with_citations)\n",
    "    else:\n",
    "        print(\"No citations found in any responses.\")\n",
    "else:\n",
    "    print(\"No citation results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Automated Evaluation - DeepEval Metrics (Optional)\n",
    "\n",
    "Uses the DeepEval library for faithfulness and answer relevancy metrics. These metrics work **without ground truth**:\n",
    "- **FaithfulnessMetric**: Are claims grounded in the cited sources?\n",
    "- **AnswerRelevancyMetric**: Does the response address the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Run DeepEval metrics\n",
    "\n",
    "try:\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    \n",
    "    DEEPEVAL_AVAILABLE = True\n",
    "    print(\"DeepEval loaded successfully.\")\n",
    "except ImportError:\n",
    "    DEEPEVAL_AVAILABLE = False\n",
    "    print(\"DeepEval not installed. Run: pip install deepeval\")\n",
    "    print(\"Skipping this section.\")\n",
    "\n",
    "deepeval_scores = []\n",
    "\n",
    "if DEEPEVAL_AVAILABLE:\n",
    "    relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    faithfulness_metric = FaithfulnessMetric(threshold=0.5)\n",
    "    \n",
    "    for result in results:\n",
    "        if not result.get(\"response_text\"):\n",
    "            continue\n",
    "        \n",
    "        print(f\"  DeepEval scoring {result['query_id']}...\", end=\" \")\n",
    "        \n",
    "        # Extract source content as retrieval context\n",
    "        sources = result.get(\"sources\", [])\n",
    "        retrieval_context = sources if isinstance(sources, list) and sources else [result[\"response_text\"][:2000]]\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=result[\"query\"],\n",
    "            actual_output=result[\"response_text\"][:5000],\n",
    "            retrieval_context=retrieval_context[:5],\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            relevancy_metric.measure(test_case)\n",
    "            faithfulness_metric.measure(test_case)\n",
    "            \n",
    "            deepeval_scores.append({\n",
    "                \"query_id\": result[\"query_id\"],\n",
    "                \"relevancy\": relevancy_metric.score,\n",
    "                \"faithfulness\": faithfulness_metric.score,\n",
    "                \"relevancy_reason\": relevancy_metric.reason,\n",
    "                \"faithfulness_reason\": faithfulness_metric.reason,\n",
    "            })\n",
    "            print(f\"rel={relevancy_metric.score:.2f}, faith={faithfulness_metric.score:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            deepeval_scores.append({\n",
    "                \"query_id\": result[\"query_id\"],\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "    \n",
    "    # Save\n",
    "    deepeval_file = os.path.join(CONFIG[\"output_dir\"], f\"deepeval_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "    with open(deepeval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(deepeval_scores, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nDeepEval scores saved to: {deepeval_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Display DeepEval scores\n",
    "\n",
    "if deepeval_scores:\n",
    "    valid_de = [s for s in deepeval_scores if \"error\" not in s]\n",
    "    df_de = pd.DataFrame(valid_de)\n",
    "    \n",
    "    if not df_de.empty:\n",
    "        print(\"=== DeepEval Scores ===\")\n",
    "        print(f\"Average Relevancy:    {df_de['relevancy'].mean():.2f}\")\n",
    "        print(f\"Average Faithfulness: {df_de['faithfulness'].mean():.2f}\")\n",
    "        print()\n",
    "        display(df_de[[\"query_id\", \"relevancy\", \"faithfulness\"]])\n",
    "    else:\n",
    "        print(\"No valid DeepEval scores.\")\n",
    "else:\n",
    "    print(\"No DeepEval scores available. Install deepeval and re-run Section 7.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Manual Verification - Freshness Check\n",
    "\n",
    "For queries in the `manual_verification` category, you can personally check if the returned data is current. Review each response and assign a freshness score:\n",
    "- **Fresh**: Data is current (within last 7 days)\n",
    "- **Slightly Stale**: Data is 1-4 weeks old\n",
    "- **Stale**: Data is 1-6 months old\n",
    "- **Outdated**: Data is 6+ months old or incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Display manual verification queries with responses\n",
    "\n",
    "manual_results = [r for r in results if r.get(\"category\") == \"manual_verification\"]\n",
    "\n",
    "if manual_results:\n",
    "    print(f\"=== Manual Verification Queries ({len(manual_results)} queries) ===\")\n",
    "    print(\"Review each response below and assign a freshness score.\\n\")\n",
    "    \n",
    "    for r in manual_results:\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style='border: 1px solid #ccc; padding: 12px; margin: 8px 0; border-radius: 4px;'>\n",
    "            <h4>[{r['query_id']}] {r['query']}</h4>\n",
    "            <p><b>Subcategory:</b> {r.get('subcategory', 'N/A')}</p>\n",
    "            <p><b>Response (first 500 chars):</b></p>\n",
    "            <pre style='white-space: pre-wrap; background: #f5f5f5; padding: 8px;'>{r.get('response_text', 'No response')[:500]}</pre>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "else:\n",
    "    print(\"No manual verification results yet. Run Section 4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Enter manual freshness scores\n",
    "# Update the scores dict below after reviewing the responses above.\n",
    "\n",
    "# Options: \"fresh\", \"slightly_stale\", \"stale\", \"outdated\"\n",
    "MANUAL_FRESHNESS_SCORES = {\n",
    "    # \"G1a\": \"fresh\",\n",
    "    # \"G1b\": \"slightly_stale\",\n",
    "    # \"G1c\": \"stale\",\n",
    "    # \"G2a\": \"fresh\",\n",
    "    # \"G3a\": \"fresh\",\n",
    "    # \"G3b\": \"outdated\",\n",
    "    # \"G5a\": \"fresh\",\n",
    "    # \"G5b\": \"slightly_stale\",\n",
    "}\n",
    "\n",
    "# Convert to numeric for aggregation\n",
    "freshness_to_score = {\"fresh\": 4, \"slightly_stale\": 3, \"stale\": 2, \"outdated\": 1}\n",
    "\n",
    "if MANUAL_FRESHNESS_SCORES:\n",
    "    freshness_data = []\n",
    "    for qid, freshness in MANUAL_FRESHNESS_SCORES.items():\n",
    "        freshness_data.append({\n",
    "            \"query_id\": qid,\n",
    "            \"freshness\": freshness,\n",
    "            \"freshness_score\": freshness_to_score.get(freshness, 0),\n",
    "        })\n",
    "    \n",
    "    df_freshness = pd.DataFrame(freshness_data)\n",
    "    print(\"=== Manual Freshness Scores ===\")\n",
    "    print(f\"Average freshness score: {df_freshness['freshness_score'].mean():.2f} / 4.0\")\n",
    "    display(df_freshness)\n",
    "    \n",
    "    # Save\n",
    "    freshness_file = os.path.join(CONFIG[\"output_dir\"], f\"freshness_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "    with open(freshness_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(freshness_data, f, indent=2)\n",
    "    print(f\"Saved to: {freshness_file}\")\n",
    "else:\n",
    "    print(\"No freshness scores entered yet. Fill in MANUAL_FRESHNESS_SCORES above and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Results Aggregation & Visualization\n",
    "\n",
    "Combine all evaluation scores and generate visual reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Aggregate all scores into a single DataFrame\n",
    "\n",
    "# Start with basic result info\n",
    "agg_data = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        \"query_id\": r[\"query_id\"],\n",
    "        \"category\": r.get(\"category\", \"\"),\n",
    "        \"subcategory\": r.get(\"subcategory\", \"\"),\n",
    "        \"timing_seconds\": r.get(\"timing_seconds\", 0),\n",
    "        \"word_count\": r.get(\"word_count\", 0),\n",
    "        \"has_error\": bool(r.get(\"error\")),\n",
    "    }\n",
    "    \n",
    "    # Merge judge scores\n",
    "    judge = next((s for s in judge_scores if s.get(\"query_id\") == r[\"query_id\"] and \"error\" not in s), None)\n",
    "    if judge:\n",
    "        for col in [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]:\n",
    "            row[f\"judge_{col}\"] = judge.get(col)\n",
    "    \n",
    "    # Merge citation scores\n",
    "    citation = next((c for c in citation_results if c[\"query_id\"] == r[\"query_id\"]), None)\n",
    "    if citation:\n",
    "        row[\"citation_count\"] = citation[\"total_citations\"]\n",
    "        row[\"citation_alive_rate\"] = citation[\"alive\"] / citation[\"total_citations\"] if citation[\"total_citations\"] > 0 else None\n",
    "        row[\"citation_support_rate\"] = citation[\"supported\"] / citation[\"alive\"] if citation[\"alive\"] > 0 else None\n",
    "    \n",
    "    # Merge DeepEval scores\n",
    "    de = next((s for s in deepeval_scores if s.get(\"query_id\") == r[\"query_id\"] and \"error\" not in s), None)\n",
    "    if de:\n",
    "        row[\"deepeval_relevancy\"] = de.get(\"relevancy\")\n",
    "        row[\"deepeval_faithfulness\"] = de.get(\"faithfulness\")\n",
    "    \n",
    "    # Merge freshness scores\n",
    "    if r[\"query_id\"] in MANUAL_FRESHNESS_SCORES:\n",
    "        row[\"freshness\"] = MANUAL_FRESHNESS_SCORES[r[\"query_id\"]]\n",
    "        row[\"freshness_score\"] = freshness_to_score.get(MANUAL_FRESHNESS_SCORES[r[\"query_id\"]], 0)\n",
    "    \n",
    "    agg_data.append(row)\n",
    "\n",
    "df_agg = pd.DataFrame(agg_data)\n",
    "\n",
    "print(f\"=== Aggregated Results ({len(df_agg)} queries) ===\")\n",
    "display(df_agg.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32: Radar chart - overall system performance\n",
    "\n",
    "judge_cols = [c for c in df_agg.columns if c.startswith(\"judge_\")]\n",
    "\n",
    "if judge_cols:\n",
    "    avg_scores = df_agg[judge_cols].mean()\n",
    "    categories_radar = [c.replace(\"judge_\", \"\").replace(\"_\", \" \").title() for c in judge_cols]\n",
    "    values = avg_scores.values.tolist()\n",
    "    values.append(values[0])  # Close the polygon\n",
    "    categories_radar.append(categories_radar[0])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories_radar,\n",
    "        fill='toself',\n",
    "        name=CONFIG[\"system_under_test\"],\n",
    "        line_color='rgb(31, 119, 180)',\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])),\n",
    "        showlegend=True,\n",
    "        title=f\"System Performance Radar - {CONFIG['system_under_test']}\",\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No judge scores available for radar chart. Run Section 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Bar charts - scores by category\n",
    "\n",
    "if judge_cols and not df_agg.empty:\n",
    "    # Average judge score per category\n",
    "    df_agg[\"judge_avg\"] = df_agg[judge_cols].mean(axis=1)\n",
    "    \n",
    "    cat_avg = df_agg.groupby(\"category\")[\"judge_avg\"].mean().sort_values(ascending=True)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=cat_avg.values,\n",
    "        y=cat_avg.index,\n",
    "        orientation='h',\n",
    "        title=f\"Average Quality Score by Category - {CONFIG['system_under_test']}\",\n",
    "        labels={\"x\": \"Average Score (1-5)\", \"y\": \"Category\"},\n",
    "    )\n",
    "    fig.update_layout(xaxis_range=[0, 5])\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No data for bar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Latency analysis\n",
    "\n",
    "if not df_agg.empty and df_agg[\"timing_seconds\"].sum() > 0:\n",
    "    fig = px.histogram(\n",
    "        df_agg, x=\"timing_seconds\", nbins=20,\n",
    "        title=f\"Response Time Distribution - {CONFIG['system_under_test']}\",\n",
    "        labels={\"timing_seconds\": \"Response Time (seconds)\"},\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Latency Statistics:\")\n",
    "    print(f\"  Mean:   {df_agg['timing_seconds'].mean():.1f}s\")\n",
    "    print(f\"  Median: {df_agg['timing_seconds'].median():.1f}s\")\n",
    "    print(f\"  P95:    {df_agg['timing_seconds'].quantile(0.95):.1f}s\")\n",
    "    print(f\"  Max:    {df_agg['timing_seconds'].max():.1f}s\")\n",
    "    \n",
    "    # Latency by category\n",
    "    fig2 = px.box(\n",
    "        df_agg, x=\"category\", y=\"timing_seconds\",\n",
    "        title=f\"Response Time by Category - {CONFIG['system_under_test']}\",\n",
    "        labels={\"timing_seconds\": \"Response Time (seconds)\", \"category\": \"Category\"},\n",
    "    )\n",
    "    fig2.show()\n",
    "else:\n",
    "    print(\"No timing data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Save final aggregated results\n",
    "\n",
    "final_json = os.path.join(CONFIG[\"output_dir\"], f\"final_aggregated_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "final_csv = os.path.join(CONFIG[\"output_dir\"], f\"final_aggregated_{CONFIG['system_under_test']}_{CONFIG['run_id']}.csv\")\n",
    "\n",
    "df_agg.to_json(final_json, orient=\"records\", indent=2)\n",
    "df_agg.to_csv(final_csv, index=False)\n",
    "\n",
    "print(f\"Final results saved:\")\n",
    "print(f\"  JSON: {final_json}\")\n",
    "print(f\"  CSV:  {final_csv}\")\n",
    "print(f\"\\nOutput directory contents:\")\n",
    "for f in sorted(os.listdir(CONFIG[\"output_dir\"])):\n",
    "    size = os.path.getsize(os.path.join(CONFIG[\"output_dir\"], f))\n",
    "    print(f\"  {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Cross-System Comparison\n",
    "\n",
    "**Run this section only after you have tested BOTH systems.**\n",
    "\n",
    "Load results from both sf EDR and Open Deep agentruns, then compare them head-to-head with pairwise LLM judging and visual overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 37: Load results from both systems\n",
    "\n",
    "# Update these paths to point to your saved result files\n",
    "sf_RESULTS_FILE = \"\"  # e.g. \"validation_results/results_sf_edr_20260220_143000.json\"\n",
    "OPEN_DR_RESULTS_FILE = \"\"     # e.g. \"validation_results/results_open_deep_research_20260221_100000.json\"\n",
    "\n",
    "if sf_RESULTS_FILE and OPEN_DR_RESULTS_FILE:\n",
    "    with open(sf_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        sf_results = json.load(f)\n",
    "    with open(OPEN_DR_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        odr_results = json.load(f)\n",
    "    \n",
    "    print(f\"sf EDR results: {len(sf_results)} queries\")\n",
    "    print(f\"Open Deep agentresults: {len(odr_results)} queries\")\n",
    "    \n",
    "    # Build lookup by query_id\n",
    "    sf_lookup = {r[\"query_id\"]: r for r in sf_results}\n",
    "    odr_lookup = {r[\"query_id\"]: r for r in odr_results}\n",
    "    common_ids = set(sf_lookup.keys()) & set(odr_lookup.keys())\n",
    "    print(f\"Common queries: {len(common_ids)}\")\n",
    "else:\n",
    "    print(\"Please set sf_RESULTS_FILE and OPEN_DR_RESULTS_FILE paths above.\")\n",
    "    print(\"Look in your validation_results/ directory for the JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Pairwise LLM comparison\n",
    "\n",
    "def pairwise_compare(query: str, response_a: str, response_b: str, config: dict) -> dict:\n",
    "    \"\"\"Ask the judge LLM to compare two responses head-to-head.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert research evaluator. Compare two research outputs for the same query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "RESPONSE A:\n",
    "{response_a[:4000]}\n",
    "\n",
    "RESPONSE B:\n",
    "{response_b[:4000]}\n",
    "\n",
    "For each dimension, indicate which response is better (A, B, or Tie):\n",
    "1. relevancy: Which better addresses the query?\n",
    "2. depth: Which is more thorough?\n",
    "3. source_quality: Which has better citations?\n",
    "4. coherence: Which is better structured?\n",
    "5. overall: Which is the better research output overall?\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"relevancy\": \"A\" or \"B\" or \"Tie\", \"depth\": \"A\" or \"B\" or \"Tie\", \"source_quality\": \"A\" or \"B\" or \"Tie\", \"coherence\": \"A\" or \"B\" or \"Tie\", \"overall\": \"A\" or \"B\" or \"Tie\", \"reasoning\": \"brief explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported provider: {provider}\"}\n",
    "        \n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Run pairwise comparison\n",
    "pairwise_results = []\n",
    "\n",
    "if 'common_ids' in dir() and common_ids:\n",
    "    for qid in sorted(common_ids):\n",
    "        sf_resp = sf_lookup[qid].get(\"response_text\", \"\")\n",
    "        odr_resp = odr_lookup[qid].get(\"response_text\", \"\")\n",
    "        \n",
    "        if not sf_resp or not odr_resp:\n",
    "            continue\n",
    "        \n",
    "        query_text = sf_lookup[qid][\"query\"]\n",
    "        print(f\"Comparing {qid}...\", end=\" \")\n",
    "        \n",
    "        comparison = pairwise_compare(query_text, sf_resp, odr_resp, CONFIG)\n",
    "        comparison[\"query_id\"] = qid\n",
    "        pairwise_results.append(comparison)\n",
    "        \n",
    "        overall = comparison.get(\"overall\", \"?\")\n",
    "        winner = \"sf\" if overall == \"A\" else \"Open DR\" if overall == \"B\" else \"Tie\"\n",
    "        print(f\"Winner: {winner}\")\n",
    "    \n",
    "    # Save\n",
    "    pairwise_file = os.path.join(CONFIG[\"output_dir\"], f\"pairwise_comparison_{CONFIG['run_id']}.json\")\n",
    "    with open(pairwise_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pairwise_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nPairwise results saved to: {pairwise_file}\")\n",
    "else:\n",
    "    print(\"Load both system results first (Cell 37).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 39: Side-by-side comparison table\n",
    "\n",
    "if pairwise_results:\n",
    "    df_pw = pd.DataFrame(pairwise_results)\n",
    "    \n",
    "    print(\"=== Pairwise Comparison Summary ===\")\n",
    "    print(f\"A = sf EDR, B = Open Deep Research\\n\")\n",
    "    \n",
    "    for dim in [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\"]:\n",
    "        if dim in df_pw.columns:\n",
    "            counts = df_pw[dim].value_counts()\n",
    "            a_wins = counts.get(\"A\", 0)\n",
    "            b_wins = counts.get(\"B\", 0)\n",
    "            ties = counts.get(\"Tie\", 0)\n",
    "            print(f\"  {dim:25s}: sf={a_wins}  Open DR={b_wins}  Tie={ties}\")\n",
    "    \n",
    "    print(f\"\\nDetailed comparison:\")\n",
    "    display(df_pw[[\"query_id\", \"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\", \"reasoning\"]])\n",
    "else:\n",
    "    print(\"No pairwise results yet. Run Cell 38 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Overlay radar chart - both systems\n",
    "\n",
    "# Load judge scores for both systems\n",
    "sf_JUDGE_FILE = \"\"  # e.g. \"validation_results/judge_scores_sf_edr_20260220_143000.json\"\n",
    "OPEN_DR_JUDGE_FILE = \"\"     # e.g. \"validation_results/judge_scores_open_deep_research_20260221_100000.json\"\n",
    "\n",
    "if sf_JUDGE_FILE and OPEN_DR_JUDGE_FILE:\n",
    "    with open(sf_JUDGE_FILE, \"r\") as f:\n",
    "        sf_judge = json.load(f)\n",
    "    with open(OPEN_DR_JUDGE_FILE, \"r\") as f:\n",
    "        odr_judge = json.load(f)\n",
    "    \n",
    "    dims = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]\n",
    "    dims_display = [d.replace(\"_\", \" \").title() for d in dims]\n",
    "    \n",
    "    sf_valid = [s for s in sf_judge if \"error\" not in s]\n",
    "    odr_valid = [s for s in odr_judge if \"error\" not in s]\n",
    "    \n",
    "    sf_avgs = [pd.DataFrame(sf_valid)[d].mean() for d in dims]\n",
    "    odr_avgs = [pd.DataFrame(odr_valid)[d].mean() for d in dims]\n",
    "    \n",
    "    # Close polygons\n",
    "    sf_avgs.append(sf_avgs[0])\n",
    "    odr_avgs.append(odr_avgs[0])\n",
    "    dims_display.append(dims_display[0])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(r=sf_avgs, theta=dims_display, fill='toself', name='sf EDR', line_color='blue'))\n",
    "    fig.add_trace(go.Scatterpolar(r=odr_avgs, theta=dims_display, fill='toself', name='Open Deep Research', line_color='red', opacity=0.6))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])),\n",
    "        title=\"Head-to-Head: sf EDR vs Open Deep Research\",\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Set sf_JUDGE_FILE and OPEN_DR_JUDGE_FILE paths to generate the overlay radar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 41: Final winner determination\n",
    "\n",
    "if pairwise_results:\n",
    "    dims = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\"]\n",
    "    weights = {\"relevancy\": 0.2, \"depth\": 0.2, \"source_quality\": 0.2, \"coherence\": 0.15, \"overall\": 0.25}\n",
    "    \n",
    "    sf_score = 0\n",
    "    odr_score = 0\n",
    "    \n",
    "    for dim in dims:\n",
    "        w = weights[dim]\n",
    "        for pw in pairwise_results:\n",
    "            if pw.get(dim) == \"A\":\n",
    "                sf_score += w\n",
    "            elif pw.get(dim) == \"B\":\n",
    "                odr_score += w\n",
    "            else:  # Tie\n",
    "                sf_score += w * 0.5\n",
    "                odr_score += w * 0.5\n",
    "    \n",
    "    total = sf_score + odr_score\n",
    "    sf_pct = sf_score / total * 100 if total > 0 else 50\n",
    "    odr_pct = odr_score / total * 100 if total > 0 else 50\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n  sf EDR:       {sf_pct:.1f}%\")\n",
    "    print(f\"  Open Deep Research:   {odr_pct:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    if sf_pct > odr_pct + 5:\n",
    "        print(f\"  WINNER: sf EDR\")\n",
    "    elif odr_pct > sf_pct + 5:\n",
    "        print(f\"  WINNER: Open Deep Research\")\n",
    "    else:\n",
    "        print(f\"  RESULT: Too close to call (within 5% margin)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Run pairwise comparison first (Cell 38).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Convention check code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup - imports and env vars\n",
    "import sys, os, logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure project root is on path\n",
    "PROJECT_ROOT = os.path.abspath(\".\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Load secrets from .env\n",
    "load_dotenv(os.path.join(PROJECT_ROOT, \".env\"), override=True)\n",
    "\n",
    "# Show what we loaded\n",
    "print(f\"LLM_PROVIDER = {os.getenv('LLM_PROVIDER')}\")\n",
    "print(f\"GEMINI_API_KEY = {os.getenv('GEMINI_API_KEY', 'NOT SET')[:10]}...\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Enable logging so we see pipeline steps\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fake research report (hardcoded - zero LLM cost)\n",
    "# Contains: a markdown table, numbers in prose, comparisons, time-series data\n",
    "\n",
    "FAKE_RESEARCH_REPORT = \"\"\"\n",
    "# Global Electric Vehicle Market Analysis 2024\n",
    "\n",
    "## Market Overview\n",
    "\n",
    "The global electric vehicle (EV) market reached $388.1 billion in 2024, growing at a CAGR of 17.8%.\n",
    "China dominated with 59% market share, followed by Europe at 25% and North America at 12%.\n",
    "\n",
    "## Sales by Region (2024)\n",
    "\n",
    "| Region | Units Sold (millions) | Market Share (%) | YoY Growth (%) |\n",
    "|--------|----------------------|------------------|----------------|\n",
    "| China | 8.9 | 59 | 22.4 |\n",
    "| Europe | 3.8 | 25 | 14.2 |\n",
    "| North America | 1.8 | 12 | 31.5 |\n",
    "| Rest of World | 0.6 | 4 | 45.1 |\n",
    "\n",
    "## Top Manufacturers by Market Cap\n",
    "\n",
    "Tesla leads with a market capitalization of $785 billion, followed by BYD at $98 billion,\n",
    "Rivian at $14 billion, and NIO at $9.2 billion. Li Auto reported $21 billion market cap\n",
    "while XPeng stood at $8.5 billion.\n",
    "\n",
    "## Battery Technology Trends\n",
    "\n",
    "Average battery pack cost declined from $153/kWh in 2022 to $139/kWh in 2023 and $128/kWh in 2024.\n",
    "Lithium iron phosphate (LFP) batteries now account for 40% of the market, up from 30% in 2023.\n",
    "Energy density improved from 250 Wh/kg in 2022 to 275 Wh/kg in 2023 and 295 Wh/kg in 2024.\n",
    "\n",
    "## Charging Infrastructure\n",
    "\n",
    "Global public charging points reached 3.9 million in 2024:\n",
    "- China: 2.7 million (69%)\n",
    "- Europe: 0.63 million (16%)\n",
    "- North America: 0.35 million (9%)\n",
    "- Rest of World: 0.22 million (6%)\n",
    "\n",
    "## Price Comparison (Average Selling Price USD)\n",
    "\n",
    "Tesla Model 3: $38,990. BYD Seal: $25,500. Hyundai Ioniq 5: $41,800.\n",
    "Volkswagen ID.4: $39,735. Nissan Leaf: $28,040. Chevrolet Equinox EV: $33,900.\n",
    "BMW iX3: $52,200. Mercedes EQA: $49,950. Kia EV6: $42,600.\n",
    "\n",
    "## Forecast\n",
    "\n",
    "The EV market is projected to reach $906.7 billion by 2028, with annual unit sales\n",
    "expected to exceed 25 million vehicles globally.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Fake report ready: {len(FAKE_RESEARCH_REPORT)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run research using the LangGraph pipeline directly (no server needed)\n",
    "# This invokes the same graph that conduct_research() uses internally\n",
    "\n",
    "import uuid\n",
    "from src.graph import create_graph\n",
    "from src.state import SummaryState\n",
    "\n",
    "# --- Configuration ---\n",
    "RESEARCH_QUERY = \"Top 5 programming languages by popularity in 2026\"  # <-- CHANGE THIS\n",
    "PROVIDER = os.getenv(\"LLM_PROVIDER\", \"gemini\")\n",
    "MODEL = os.getenv(\"LLM_MODEL\", None)\n",
    "\n",
    "print(f\"Query:    {RESEARCH_QUERY}\")\n",
    "print(f\"Provider: {PROVIDER}\")\n",
    "print(f\"Model:    {MODEL}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create graph and initial state (mirrors conduct_research logic)\n",
    "graph = create_graph()\n",
    "\n",
    "initial_state = SummaryState(\n",
    "    research_topic=RESEARCH_QUERY,\n",
    "    search_query=RESEARCH_QUERY,\n",
    "    running_summary=\"\",\n",
    "    research_complete=False,\n",
    "    knowledge_gap=\"\",\n",
    "    research_loop_count=0,\n",
    "    sources_gathered=[],\n",
    "    web_research_results=[],\n",
    "    search_results_empty=False,\n",
    "    selected_search_tool=\"general_search\",\n",
    "    source_citations={},\n",
    "    subtopic_queries=[],\n",
    "    subtopics_metadata=[],\n",
    "    extra_effort=False,\n",
    "    minimum_effort=True,  # Keep it cheap\n",
    "    benchmark_mode=False,\n",
    "    llm_provider=PROVIDER,\n",
    "    llm_model=MODEL,\n",
    "    uploaded_knowledge=None,\n",
    "    uploaded_files=[],\n",
    "    steering_enabled=False,\n",
    "    parallel_search_enabled=False,\n",
    "    parallel_search_max_concurrency=2,\n",
    "    database_info=None,\n",
    "    analysis_required=False,  # We'll run analysis separately in the next cell\n",
    ")\n",
    "\n",
    "graph_config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"stream_id\": str(uuid.uuid4()),\n",
    "        \"llm_provider\": PROVIDER,\n",
    "        \"llm_model\": MODEL,\n",
    "        \"user_prompt\": RESEARCH_QUERY,\n",
    "        \"database_info\": None,\n",
    "    },\n",
    "    \"recursion_limit\": 100,\n",
    "}\n",
    "\n",
    "print(\"Starting research (this may take a few minutes)...\")\n",
    "import time\n",
    "research_start = time.time()\n",
    "\n",
    "# ainvoke returns the final state as a dict\n",
    "final_state = await graph.ainvoke(initial_state, graph_config)\n",
    "\n",
    "research_time = time.time() - research_start\n",
    "print(f\"\\nResearch completed in {research_time:.1f}s\")\n",
    "\n",
    "# Extract the research report\n",
    "research_report = final_state.get(\"markdown_report\") or final_state.get(\"running_summary\") or \"\"\n",
    "print(f\"Report length: {len(research_report)} chars\")\n",
    "print(f\"Sources: {len(final_state.get('sources_gathered', []))}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the research report\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "if research_report:\n",
    "    print(f\"Report: {len(research_report)} chars\\n\")\n",
    "    # Show first 2000 chars as rendered markdown\n",
    "    display(Markdown(research_report[:2000]))\n",
    "    if len(research_report) > 2000:\n",
    "        print(f\"\\n... (truncated, full report is {len(research_report)} chars)\")\n",
    "else:\n",
    "    print(\"No research report generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass research report to the analysis pipeline (~3 LLM calls)\n",
    "\n",
    "from extensions.agents.data_analysis_agent import DataAnalysisAgent\n",
    "\n",
    "if not research_report or len(research_report) < 100:\n",
    "    print(\"Research report is too short or empty. Skipping analysis.\")\n",
    "else:\n",
    "    print(f\"Passing {len(research_report)} chars of research to analysis pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    agent = DataAnalysisAgent(provider=PROVIDER, model=MODEL)\n",
    "    analysis_result = agent.run_pipeline(research_report)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Status:          {analysis_result['status']}\")\n",
    "    print(f\"Error:           {analysis_result['error']}\")\n",
    "    print(f\"Execution time:  {analysis_result['execution_time']:.1f}s\")\n",
    "    print(f\"Charts created:  {len(analysis_result['charts'])}\")\n",
    "    print(f\"Explanations:    {len(analysis_result['chart_explanations'])}\")\n",
    "    print(f\"Outlier analysis:{' Yes' if analysis_result.get('outlier_analysis') else ' No'}\")\n",
    "    print(f\"Output:          {analysis_result['output']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Show extracted data\n",
    "    extracted = analysis_result.get(\"extracted_data\", \"\")\n",
    "    if extracted:\n",
    "        print(\"\\n=== EXTRACTED DATA (first 1500 chars) ===\")\n",
    "        print(extracted[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize agent and run pipeline (~3 LLM calls)\n",
    "\n",
    "from extensions.agents.data_analysis_agent import DataAnalysisAgent\n",
    "\n",
    "provider = os.getenv(\"LLM_PROVIDER\", \"gemini\")\n",
    "model = os.getenv(\"LLM_MODEL\", None)\n",
    "print(f\"Initializing agent with provider={provider}, model={model}\")\n",
    "\n",
    "agent = DataAnalysisAgent(provider=provider, model=model)\n",
    "print(\"Agent initialized. Running pipeline...\\n\")\n",
    "\n",
    "result = agent.run_pipeline(FAKE_RESEARCH_REPORT)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Status:          {result['status']}\")\n",
    "print(f\"Error:           {result['error']}\")\n",
    "print(f\"Execution time:  {result['execution_time']:.1f}s\")\n",
    "print(f\"Charts created:  {len(result['charts'])}\")\n",
    "print(f\"Explanations:    {len(result['chart_explanations'])}\")\n",
    "print(f\"Outlier analysis:{' Yes' if result.get('outlier_analysis') else ' No'}\")\n",
    "print(f\"Output:          {result['output']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Inspect extracted data and profile\n",
    "\n",
    "print(\"=== EXTRACTED DATA ===\")\n",
    "extracted = result.get(\"extracted_data\", \"\")\n",
    "if extracted:\n",
    "    print(extracted[:2000])\n",
    "else:\n",
    "    print(\"(no data extracted)\")\n",
    "\n",
    "print(\"\\n=== DATA PROFILE (first 1000 chars) ===\")\n",
    "profile = result.get(\"data_profile\", \"\")\n",
    "if profile:\n",
    "    print(profile[:1000])\n",
    "else:\n",
    "    print(\"(no profile)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate chart files exist on disk\n",
    "\n",
    "charts = result.get(\"charts\", [])\n",
    "print(f\"Charts: {len(charts)}\")\n",
    "for i, path in enumerate(charts, 1):\n",
    "    exists = os.path.exists(path)\n",
    "    size = os.path.getsize(path) if exists else 0\n",
    "    status = f\"{size:,} bytes\" if exists else \"MISSING\"\n",
    "    print(f\"  {i}. {path} [{status}]\")\n",
    "\n",
    "print(f\"\\nChart Explanations:\")\n",
    "for path, info in result.get(\"chart_explanations\", {}).items():\n",
    "    print(f\"  - {info['title']}: {info['explanation'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Display charts inline (Plotly HTML renders in Jupyter)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "charts = result.get(\"charts\", [])\n",
    "if not charts:\n",
    "    print(\"No charts to display.\")\n",
    "else:\n",
    "    for i, path in enumerate(charts, 1):\n",
    "        if os.path.exists(path):\n",
    "            title = result.get(\"chart_explanations\", {}).get(path, {}).get(\"title\", f\"Chart {i}\")\n",
    "            print(f\"\\n--- {title} ---\")\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                html_content = f.read()\n",
    "            # Wrap in iframe to isolate Plotly JS\n",
    "            iframe = f'<iframe srcdoc=\"{html_content.replace(chr(34), \"&quot;\")}\" width=\"100%\" height=\"500\" frameborder=\"0\"></iframe>'\n",
    "            display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test HTML report builder (zero LLM calls - just builds HTML)\n",
    "\n",
    "charts = result.get(\"charts\", [])\n",
    "if charts:\n",
    "    from extensions.utils.report_builder import build_html_report\n",
    "\n",
    "    report_path = build_html_report(\n",
    "        display_text=FAKE_RESEARCH_REPORT,\n",
    "        analysis_output=result[\"output\"],\n",
    "        figures=result[\"charts\"],\n",
    "        chart_explanations=result[\"chart_explanations\"],\n",
    "        sources=[],\n",
    "        query=\"EV Market Analysis Test\",\n",
    "        sub_queries=[],\n",
    "        extracted_data_summary=result.get(\"extracted_data\", \"\"),\n",
    "        data_profile_summary=result.get(\"data_profile\", \"\"),\n",
    "    )\n",
    "    print(f\"HTML report generated: {report_path}\")\n",
    "    print(f\"File size: {os.path.getsize(report_path):,} bytes\")\n",
    "    print(f\"\\nOpen in browser: file:///{os.path.abspath(report_path).replace(os.sep, '/')}\")\n",
    "else:\n",
    "    print(\"Skipping report builder - no charts were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test intent detection (~4 LLM calls)\n",
    "# Verifies that analysis_required=True for analysis queries, False for research-only\n",
    "\n",
    "import asyncio\n",
    "from services.intent_detector import detect_intent\n",
    "\n",
    "test_queries = [\n",
    "    (\"Research AI market trends\", False),\n",
    "    (\"Research AI market trends and analyze with charts\", True),\n",
    "    (\"Tell me about quantum computing\", False),\n",
    "    (\"Pollution data - analyze and visualize\", True),\n",
    "    (\"Research on Environment Pollution  - Generate visualizations\", True),\n",
    "\n",
    "]\n",
    "\n",
    "print(\"Intent Detection Tests:\")\n",
    "print(\"-\" * 70)\n",
    "for query, expected in test_queries:\n",
    "    result_intent = await detect_intent(query, provider=provider, model=model)\n",
    "    actual = result_intent[\"analysis_required\"]\n",
    "    match = \"PASS\" if actual == expected else \"FAIL\"\n",
    "    print(f\"  [{match}] '{query[:50]}' -> {actual} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary - pass/fail checklist\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"Pipeline completed without error\", result[\"status\"] == \"completed\"),\n",
    "    (\"Data was extracted\", bool(result.get(\"extracted_data\", \"\").strip())),\n",
    "    (\"Data was profiled\", bool(result.get(\"data_profile\", \"\").strip())),\n",
    "    (\"At least 1 chart created\", len(result.get(\"charts\", [])) >= 1),\n",
    "    (\"All chart files exist on disk\", all(os.path.exists(p) for p in result.get(\"charts\", []))),\n",
    "    (\"Chart explanations populated\", len(result.get(\"chart_explanations\", {})) >= 1),\n",
    "    (\"Execution time recorded\", result.get(\"execution_time\", 0) > 0),\n",
    "]\n",
    "\n",
    "all_pass = True\n",
    "for desc, passed in checks:\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "    print(f\"  [{status}] {desc}\")\n",
    "\n",
    "print(\"\\n\" + (\"ALL CHECKS PASSED\" if all_pass else \"SOME CHECKS FAILED\"))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
