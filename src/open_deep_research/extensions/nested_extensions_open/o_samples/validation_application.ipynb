{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Deep Research Validation Framework\n",
    "\n",
    "This notebook provides a comprehensive framework for evaluating the Deep agent application.\n",
    "\n",
    "**System Under Test:**\n",
    "- Deep agent\n",
    "\n",
    "**Approach:** Sequential testing with incremental result saving. No ground truth required.\n",
    "\n",
    "**Evaluation Methods:**\n",
    "1. LLM-as-Judge (automated quality scoring)\n",
    "2. Citation Verification (URL liveness + content support)\n",
    "3. DeepEval Metrics (faithfulness, relevancy)\n",
    "4. Manual Verification (freshness/recency checks)\n",
    "5. Cross-System Comparison (compare two validation runs head-to-head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install dependencies\n",
    "# Uncomment and run if not already installed\n",
    "\n",
    "# !pip install deepeval ragas openai anthropic requests beautifulsoup4 plotly pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports and environment setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path(r\"xx\\open_deep_research-main\\open_deep_research-main\")\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Import MasterAgent and utilities\n",
    "from extensions.agents.master_agent import MasterAgent\n",
    "from extensions.utils.report_builder import build_html_report\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Source dir:   {SRC_DIR}\")\n",
    "print(f\"Environment loaded from: {PROJECT_ROOT / '.env'}\")\n",
    "print(f\"MasterAgent imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "\n",
    "CONFIG = {\n",
    "    # System under test\n",
    "    \"system_under_test\": \"open_deep_research\",\n",
    "    \n",
    "    # Open Deep Research settings\n",
    "    \"odr_use_enhanced_research\": False,   # Set True for 3-4x more comprehensive (slower)\n",
    "    \"odr_provider\": os.environ.get(\"LLM_PROVIDER\", \"google\"),\n",
    "    \"odr_model\": os.environ.get(\"LLM_MODEL\", \"gemini-2.5-pro\"),\n",
    "    \n",
    "    # LLM Judge settings (use a different LLM than the system being tested)\n",
    "    \"judge_provider\": \"openai\",   # or \"anthropic\"\n",
    "    \"judge_model\": \"gpt-4o\",      # or \"claude-sonnet-4-5-20250929\"\n",
    "    \n",
    "    # Output settings\n",
    "    \"output_dir\": str(PROJECT_ROOT / \"validation_results\"),\n",
    "    \"run_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Test Query Bank\n",
    "\n",
    "All test queries organized by category. Each query has:\n",
    "- `id`: Unique identifier\n",
    "- `query`: The research question\n",
    "- `category`: High-level category (factual, stress, manual_verification, etc.)\n",
    "- `subcategory`: Specific test type within the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define all test queries\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    # === A. Factual / Verifiable Queries ===\n",
    "    {\"id\": \"A1\", \"query\": \"What were the top 5 causes of the 2008 financial crisis, with supporting data?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A2\", \"query\": \"List all Nobel Prize winners in Physics from 2020-2024 and their contributions.\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A3\", \"query\": \"What is the current market share of cloud providers (AWS, Azure, GCP) as of 2024?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A4\", \"query\": \"Summarize the key provisions of the EU AI Act.\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    {\"id\": \"A5\", \"query\": \"What are the FDA-approved treatments for Type 2 diabetes as of 2024?\", \"category\": \"factual\", \"subcategory\": \"verifiable\"},\n",
    "    \n",
    "    # === B. Multi-Hop Reasoning Queries ===\n",
    "    {\"id\": \"B1\", \"query\": \"How did the semiconductor chip shortage of 2020-2023 affect both the automotive and gaming industries differently?\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    {\"id\": \"B2\", \"query\": \"Compare the economic policies of the US and EU regarding AI regulation and their downstream effects on startup funding.\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    {\"id\": \"B3\", \"query\": \"Trace the supply chain of lithium from mining to EV batteries - what are the geopolitical risks at each stage?\", \"category\": \"multi_hop\", \"subcategory\": \"synthesis\"},\n",
    "    \n",
    "    # === C. Ambiguous / Open-Ended Queries ===\n",
    "    {\"id\": \"C1\", \"query\": \"What's the best programming language?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    {\"id\": \"C2\", \"query\": \"Is AI dangerous?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    {\"id\": \"C3\", \"query\": \"Tell me about the Apple situation\", \"category\": \"ambiguous\", \"subcategory\": \"ambiguous_entity\"},\n",
    "    {\"id\": \"C4\", \"query\": \"What happened recently in tech?\", \"category\": \"ambiguous\", \"subcategory\": \"vague\"},\n",
    "    \n",
    "    # === D. Stress Tests ===\n",
    "    # D1: Contradictory sources\n",
    "    {\"id\": \"D1a\", \"query\": \"Is coffee good or bad for health? Provide evidence for both sides.\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    {\"id\": \"D1b\", \"query\": \"Is remote work more productive than in-office? What does the research say?\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    {\"id\": \"D1c\", \"query\": \"Are electric vehicles truly better for the environment when considering full lifecycle?\", \"category\": \"stress\", \"subcategory\": \"contradictory\"},\n",
    "    \n",
    "    # D2: Obscure topics\n",
    "    {\"id\": \"D2a\", \"query\": \"What is the history of the Voynich Manuscript's ownership chain?\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2b\", \"query\": \"Describe the political structure of the Principality of Sealand.\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2c\", \"query\": \"What are the known side effects of the drug Zuranolone in postpartum depression?\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    {\"id\": \"D2d\", \"query\": \"Summarize the contributions of Srinivasa Ramanujan's lost notebook to number theory.\", \"category\": \"stress\", \"subcategory\": \"obscure\"},\n",
    "    \n",
    "    # D3: Very recent events\n",
    "    {\"id\": \"D3a\", \"query\": \"What were the most significant tech industry events in the past 48 hours?\", \"category\": \"stress\", \"subcategory\": \"freshness\"},\n",
    "    {\"id\": \"D3b\", \"query\": \"What is the latest stock price movement for NVIDIA and why?\", \"category\": \"stress\", \"subcategory\": \"freshness\"},\n",
    "    \n",
    "    # D4: Highly technical\n",
    "    {\"id\": \"D4a\", \"query\": \"Explain the differences between LoRA, QLoRA, and DoRA fine-tuning methods with benchmark comparisons.\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    {\"id\": \"D4b\", \"query\": \"Compare the architectures of Mamba, RWKV, and Transformer models for sequence modeling.\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    {\"id\": \"D4c\", \"query\": \"What is the current state of topological quantum computing at Microsoft and IBM?\", \"category\": \"stress\", \"subcategory\": \"technical\"},\n",
    "    \n",
    "    # D5: Long-form output\n",
    "    {\"id\": \"D5a\", \"query\": \"Write a comprehensive 5000-word research report on quantum computing's impact on cryptography.\", \"category\": \"stress\", \"subcategory\": \"long_form\"},\n",
    "    \n",
    "    # D7: Edge cases\n",
    "    {\"id\": \"D7a\", \"query\": \"research\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    {\"id\": \"D7b\", \"query\": \"asdfghjkl qwerty zxcvbnm research this\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    {\"id\": \"D7c\", \"query\": \"Write a short but comprehensive 10,000-word summary\", \"category\": \"stress\", \"subcategory\": \"edge_case\"},\n",
    "    \n",
    "    # D9: Multi-language\n",
    "    {\"id\": \"D9a\", \"query\": \"Summarize the key findings of recent Chinese AI research papers on large language models.\", \"category\": \"stress\", \"subcategory\": \"multi_language\"},\n",
    "    {\"id\": \"D9b\", \"query\": \"What are the latest German automotive industry reports on EV adoption?\", \"category\": \"stress\", \"subcategory\": \"multi_language\"},\n",
    "    \n",
    "    # D10: Numerical accuracy\n",
    "    {\"id\": \"D10a\", \"query\": \"What were the exact GDP growth rates for G7 countries in 2024 Q1-Q4?\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    {\"id\": \"D10b\", \"query\": \"List the top 10 most funded AI startups in 2024 with their exact funding amounts.\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    {\"id\": \"D10c\", \"query\": \"What are the current interest rates set by the Fed, ECB, and Bank of Japan?\", \"category\": \"stress\", \"subcategory\": \"numerical\"},\n",
    "    \n",
    "    # === E. Domain-Specific Queries ===\n",
    "    {\"id\": \"E1\", \"query\": \"What are the key differences between GDPR and CCPA?\", \"category\": \"domain_specific\", \"subcategory\": \"legal\"},\n",
    "    {\"id\": \"E2\", \"query\": \"What is the current evidence on intermittent fasting for cardiovascular health?\", \"category\": \"domain_specific\", \"subcategory\": \"medical\"},\n",
    "    {\"id\": \"E3\", \"query\": \"Analyze Tesla's Q3 2024 earnings - what are the key takeaways?\", \"category\": \"domain_specific\", \"subcategory\": \"financial\"},\n",
    "    {\"id\": \"E4\", \"query\": \"What is the current state of nuclear fusion research?\", \"category\": \"domain_specific\", \"subcategory\": \"scientific\"},\n",
    "    \n",
    "    # === F. Recent Research Generation ===\n",
    "    {\"id\": \"F1\", \"query\": \"Find and summarize the 5 most recent research papers on LLM hallucination mitigation published in 2025-2026.\", \"category\": \"recent_research\", \"subcategory\": \"papers\"},\n",
    "    {\"id\": \"F2\", \"query\": \"What are the latest breakthroughs in solid-state batteries from the past 6 months?\", \"category\": \"recent_research\", \"subcategory\": \"breakthroughs\"},\n",
    "    {\"id\": \"F3\", \"query\": \"Summarize recent clinical trial results for GLP-1 receptor agonists in 2025.\", \"category\": \"recent_research\", \"subcategory\": \"clinical_trials\"},\n",
    "    \n",
    "    # === G. Manual Verification Queries (Freshness Check) ===\n",
    "    # G1: Live/real-time data\n",
    "    {\"id\": \"G1a\", \"query\": \"What is today's price of Bitcoin?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    {\"id\": \"G1b\", \"query\": \"What is the current USD to INR exchange rate?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    {\"id\": \"G1c\", \"query\": \"What is NVIDIA's stock price right now?\", \"category\": \"manual_verification\", \"subcategory\": \"realtime\"},\n",
    "    \n",
    "    # G2: Recent events\n",
    "    {\"id\": \"G2a\", \"query\": \"What were the top tech news stories this week?\", \"category\": \"manual_verification\", \"subcategory\": \"recent_events\"},\n",
    "    \n",
    "    # G3: Recently changed facts\n",
    "    {\"id\": \"G3a\", \"query\": \"What is the latest version of Python?\", \"category\": \"manual_verification\", \"subcategory\": \"changed_facts\"},\n",
    "    {\"id\": \"G3b\", \"query\": \"What is the current US federal interest rate?\", \"category\": \"manual_verification\", \"subcategory\": \"changed_facts\"},\n",
    "    \n",
    "    # G5: Trick questions\n",
    "    {\"id\": \"G5a\", \"query\": \"What is the latest iPhone model?\", \"category\": \"manual_verification\", \"subcategory\": \"trick\"},\n",
    "    {\"id\": \"G5b\", \"query\": \"What is the most recent SpaceX Starship launch result?\", \"category\": \"manual_verification\", \"subcategory\": \"trick\"},\n",
    "]\n",
    "\n",
    "# Summary\n",
    "df_queries = pd.DataFrame(TEST_QUERIES)\n",
    "print(f\"Total test queries: {len(TEST_QUERIES)}\")\n",
    "print(f\"\\nQueries by category:\")\n",
    "print(df_queries['category'].value_counts().to_string())\n",
    "print(f\"\\nQueries by subcategory:\")\n",
    "print(df_queries['subcategory'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Research Runner (Adapter Pattern)\n",
    "\n",
    "One standardized interface backed by MasterAgent. The adapter returns:\n",
    "```python\n",
    "{\n",
    "    \"query\": str,\n",
    "    \"response_text\": str,\n",
    "    \"sources\": List[str],\n",
    "    \"timing_seconds\": float,\n",
    "    \"word_count\": int,\n",
    "    \"metadata\": dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: SF Enterprise EDR Adapter (DISABLED)\n",
    "# Set SF_EDR_AVAILABLE = True and uncomment the import if you have the SF EDR system\n",
    "# and want to run cross-system comparison.\n",
    "\n",
    "SF_EDR_AVAILABLE = False\n",
    "\n",
    "async def run_sf_edr(query: str, config: dict) -> dict:\n",
    "    \"\"\"Run a query through SF Enterprise Deep Research.\n",
    "    \n",
    "    Requires benchmarks/run_research.py which is not part of this repository.\n",
    "    Enable SF_EDR_AVAILABLE and configure the import if available.\n",
    "    \"\"\"\n",
    "    if not SF_EDR_AVAILABLE:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": 0,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": \"SF EDR adapter not available in this repository. Set SF_EDR_AVAILABLE = True and configure the import.\",\n",
    "            \"metadata\": {\"system\": \"sf_edr\"}\n",
    "        }\n",
    "    \n",
    "    # Uncomment below if SF EDR is available:\n",
    "    # from benchmarks.run_research import run_research_sync\n",
    "    # ... original implementation ...\n",
    "    \n",
    "print(\"SF EDR adapter: DISABLED (set SF_EDR_AVAILABLE = True to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Open Deep Research Adapter (MasterAgent)\n",
    "\n",
    "# Singleton MasterAgent instance -- reused across all 45 queries to avoid re-initialization overhead\n",
    "_master_agent = None\n",
    "\n",
    "def _get_master_agent(config: dict) -> MasterAgent:\n",
    "    \"\"\"Lazily initialize a shared MasterAgent instance.\"\"\"\n",
    "    global _master_agent\n",
    "    if _master_agent is None:\n",
    "        _master_agent = MasterAgent(\n",
    "            use_enhanced_research=config.get(\"odr_use_enhanced_research\", False),\n",
    "            provider=config.get(\"odr_provider\"),\n",
    "            model=config.get(\"odr_model\"),\n",
    "            enable_state_persistence=False,  # No DB persistence needed during validation\n",
    "        )\n",
    "        print(f\"[Adapter] MasterAgent initialized (enhanced={config.get('odr_use_enhanced_research', False)})\")\n",
    "    return _master_agent\n",
    "\n",
    "\n",
    "async def run_open_deep_research(query: str, config: dict) -> dict:\n",
    "    \"\"\"Run a query through Open Deep Research via MasterAgent.\n",
    "    \n",
    "    Returns the standardized adapter format consumed by all downstream evaluation sections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        agent = _get_master_agent(config)\n",
    "        result = await agent.run_async(query)\n",
    "        \n",
    "        state = result.get(\"state\", {})\n",
    "        status = result.get(\"status\", \"error\")\n",
    "        elapsed = result.get(\"execution_time\", 0)\n",
    "        \n",
    "        response_text = state.get(\"final_report\", \"\")\n",
    "        \n",
    "        # Sources come as List[str] of URLs\n",
    "        sources_raw = state.get(\"sources\", [])\n",
    "        sources = []\n",
    "        for s in sources_raw:\n",
    "            if isinstance(s, str):\n",
    "                sources.append(s)\n",
    "            elif isinstance(s, dict):\n",
    "                sources.append(s.get(\"url\", s.get(\"title\", str(s))))\n",
    "        \n",
    "        base_metadata = {\n",
    "            \"system\": \"open_deep_research\",\n",
    "            \"provider\": config.get(\"odr_provider\"),\n",
    "            \"model\": config.get(\"odr_model\"),\n",
    "            \"conversation_id\": result.get(\"conversation_id\", \"\"),\n",
    "            \"agents_used\": result.get(\"agents_used\", []),\n",
    "        }\n",
    "        \n",
    "        if status == \"error\":\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response_text\": response_text,\n",
    "                \"sources\": sources,\n",
    "                \"timing_seconds\": elapsed,\n",
    "                \"word_count\": len(response_text.split()) if response_text else 0,\n",
    "                \"error\": result.get(\"error\", \"Unknown error\"),\n",
    "                \"metadata\": base_metadata,\n",
    "            }\n",
    "        \n",
    "        # Include analysis artifacts in metadata for rich validation\n",
    "        base_metadata.update({\n",
    "            \"analysis_output\": state.get(\"analysis_output\", \"\"),\n",
    "            \"charts\": state.get(\"charts\", []),\n",
    "            \"chart_explanations\": state.get(\"chart_explanations\", {}),\n",
    "            \"extracted_data\": state.get(\"extracted_data\", \"\"),\n",
    "            \"data_profile\": state.get(\"data_profile\", \"\"),\n",
    "            \"sub_queries\": state.get(\"sub_queries\", []),\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": response_text,\n",
    "            \"sources\": sources,\n",
    "            \"timing_seconds\": elapsed,\n",
    "            \"word_count\": len(response_text.split()) if response_text else 0,\n",
    "            \"metadata\": base_metadata,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": 0,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"metadata\": {\n",
    "                \"system\": \"open_deep_research\",\n",
    "                \"provider\": config.get(\"odr_provider\"),\n",
    "                \"model\": config.get(\"odr_model\"),\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Open Deep Research adapter ready (MasterAgent).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Dispatcher\n",
    "\n",
    "async def run_research(query: str, system: str, config: dict) -> dict:\n",
    "    \"\"\"Route a research query to the appropriate system adapter.\"\"\"\n",
    "    adapters = {\n",
    "        \"open_deep_research\": run_open_deep_research,\n",
    "    }\n",
    "    \n",
    "    # Include SF EDR only if available\n",
    "    if SF_EDR_AVAILABLE:\n",
    "        adapters[\"sf_edr\"] = run_sf_edr\n",
    "    \n",
    "    adapter = adapters.get(system)\n",
    "    if adapter is None:\n",
    "        raise ValueError(f\"Unknown system: {system}. Choose from: {list(adapters.keys())}\")\n",
    "    \n",
    "    return await adapter(query, config)\n",
    "\n",
    "print(f\"Dispatcher ready. System under test: {CONFIG['system_under_test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Execute Research Queries\n",
    "\n",
    "Run all test queries through the selected system. Results are saved incrementally to JSON after each query, so no data is lost if the process is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Run all queries\n",
    "\n",
    "RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"results_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "# Load existing results if resuming a partial run\n",
    "if os.path.exists(RESULTS_FILE):\n",
    "    with open(RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "    completed_ids = {r[\"query_id\"] for r in results}\n",
    "    print(f\"Resuming: {len(completed_ids)} queries already completed\")\n",
    "else:\n",
    "    results = []\n",
    "    completed_ids = set()\n",
    "\n",
    "# Filter to queries not yet completed\n",
    "pending_queries = [q for q in TEST_QUERIES if q[\"id\"] not in completed_ids]\n",
    "print(f\"Queries to run: {len(pending_queries)} / {len(TEST_QUERIES)}\")\n",
    "\n",
    "for i, query_info in enumerate(pending_queries):\n",
    "    print(f\"\\n[{i+1}/{len(pending_queries)}] Running: {query_info['id']} - {query_info['query'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = await run_research(\n",
    "            query=query_info[\"query\"],\n",
    "            system=CONFIG[\"system_under_test\"],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        \n",
    "        # Attach query metadata\n",
    "        result[\"query_id\"] = query_info[\"id\"]\n",
    "        result[\"category\"] = query_info[\"category\"]\n",
    "        result[\"subcategory\"] = query_info[\"subcategory\"]\n",
    "        result[\"system\"] = CONFIG[\"system_under_test\"]\n",
    "        result[\"timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Incremental save\n",
    "        with open(RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        status = \"ERROR\" if result.get(\"error\") else \"OK\"\n",
    "        print(f\"  [{status}] {result['word_count']} words, {result['timing_seconds']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [FAILED] {e}\")\n",
    "        results.append({\n",
    "            \"query_id\": query_info[\"id\"],\n",
    "            \"query\": query_info[\"query\"],\n",
    "            \"category\": query_info[\"category\"],\n",
    "            \"subcategory\": query_info[\"subcategory\"],\n",
    "            \"system\": CONFIG[\"system_under_test\"],\n",
    "            \"response_text\": \"\",\n",
    "            \"sources\": [],\n",
    "            \"timing_seconds\": 0,\n",
    "            \"word_count\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        })\n",
    "        with open(RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nAll queries complete. Results saved to: {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Quick summary of results\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "total = len(df_results)\n",
    "\n",
    "# Safe error counting - handle missing 'error' column\n",
    "if 'error' in df_results.columns:\n",
    "    error_mask = df_results['error'].notna() & (df_results['error'] != '')\n",
    "    error_count = int(error_mask.sum())\n",
    "else:\n",
    "    error_count = 0\n",
    "\n",
    "success_count = total - error_count\n",
    "\n",
    "print(f\"=== Execution Summary ===\")\n",
    "print(f\"Total queries run:   {total}\")\n",
    "print(f\"Successful:          {success_count}\")\n",
    "print(f\"Errors:              {error_count}\")\n",
    "print(f\"Average latency:     {df_results['timing_seconds'].mean():.1f}s\")\n",
    "print(f\"Median latency:      {df_results['timing_seconds'].median():.1f}s\")\n",
    "print(f\"Average word count:  {df_results['word_count'].mean():.0f}\")\n",
    "print(f\"\\nLatency by category:\")\n",
    "print(df_results.groupby('category')['timing_seconds'].agg(['mean', 'median', 'max']).round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Automated Evaluation - LLM-as-Judge\n",
    "\n",
    "Uses a strong LLM (GPT-4 / Claude) to automatically score each research output on multiple dimensions. **No ground truth required** - the judge evaluates standalone quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: LLM-as-Judge scoring function\n",
    "\n",
    "def get_judge_client(config: dict):\n",
    "    \"\"\"Initialize the judge LLM client.\"\"\"\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    if provider == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        return OpenAI(), config.get(\"judge_model\", \"gpt-4o\")\n",
    "    elif provider == \"anthropic\":\n",
    "        from anthropic import Anthropic\n",
    "        return Anthropic(), config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported judge provider: {provider}\")\n",
    "\n",
    "\n",
    "def llm_judge_score(query: str, response: str, config: dict) -> dict:\n",
    "    \"\"\"Score a research response using an LLM judge. No ground truth needed.\"\"\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"You are an expert research quality evaluator. Score the following research output.\n",
    "There is NO ground truth - evaluate the response on its own merits.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "RESPONSE:\n",
    "{response[:8000]}\n",
    "\n",
    "Score each dimension on a 1-5 scale (5 is best):\n",
    "\n",
    "1. relevancy (1-5): Does the response directly address the query?\n",
    "2. depth (1-5): How thorough and comprehensive is the coverage?\n",
    "3. source_quality (1-5): Are citations from reputable, relevant sources? Are sources properly referenced?\n",
    "4. coherence (1-5): Is the response well-structured, logical, and readable?\n",
    "5. confidence_calibration (1-5): Does it appropriately express uncertainty where warranted? (5 = good calibration)\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no explanation outside the JSON):\n",
    "{{\"relevancy\": X, \"depth\": X, \"source_quality\": X, \"coherence\": X, \"confidence_calibration\": X, \"reasoning\": \"brief 1-2 sentence explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    \n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported judge provider: {provider}\"}\n",
    "        \n",
    "        # Parse JSON from response (handle markdown code blocks)\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        \n",
    "        return json.loads(raw)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(\"LLM-as-Judge function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Run LLM-as-Judge on all results\n",
    "\n",
    "JUDGE_RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"judge_scores_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "# Load existing judge scores if resuming\n",
    "if os.path.exists(JUDGE_RESULTS_FILE):\n",
    "    with open(JUDGE_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        judge_scores = json.load(f)\n",
    "    judged_ids = {s[\"query_id\"] for s in judge_scores}\n",
    "    print(f\"Resuming: {len(judged_ids)} already judged\")\n",
    "else:\n",
    "    judge_scores = []\n",
    "    judged_ids = set()\n",
    "\n",
    "pending = [r for r in results if r[\"query_id\"] not in judged_ids and r.get(\"response_text\")]\n",
    "print(f\"Queries to judge: {len(pending)}\")\n",
    "\n",
    "for i, result in enumerate(pending):\n",
    "    print(f\"  [{i+1}/{len(pending)}] Judging {result['query_id']}...\", end=\" \")\n",
    "    \n",
    "    score = llm_judge_score(result[\"query\"], result[\"response_text\"], CONFIG)\n",
    "    score[\"query_id\"] = result[\"query_id\"]\n",
    "    score[\"category\"] = result[\"category\"]\n",
    "    score[\"subcategory\"] = result[\"subcategory\"]\n",
    "    \n",
    "    judge_scores.append(score)\n",
    "    \n",
    "    # Incremental save\n",
    "    with open(JUDGE_RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(judge_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    if \"error\" in score:\n",
    "        print(f\"ERROR: {score['error']}\")\n",
    "    else:\n",
    "        avg = (score[\"relevancy\"] + score[\"depth\"] + score[\"source_quality\"] + score[\"coherence\"] + score[\"confidence_calibration\"]) / 5\n",
    "        print(f\"avg={avg:.1f}\")\n",
    "\n",
    "print(f\"\\nJudge scores saved to: {JUDGE_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Display judge scores\n",
    "\n",
    "valid_scores = [s for s in judge_scores if \"error\" not in s]\n",
    "df_scores = pd.DataFrame(valid_scores)\n",
    "\n",
    "score_cols = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]\n",
    "\n",
    "if not df_scores.empty:\n",
    "    # Overall averages\n",
    "    print(\"=== Overall LLM-as-Judge Scores ===\")\n",
    "    print(df_scores[score_cols].mean().round(2).to_string())\n",
    "    \n",
    "    # By category\n",
    "    print(\"\\n=== Scores by Category ===\")\n",
    "    display(df_scores.groupby(\"category\")[score_cols].mean().round(2))\n",
    "    \n",
    "    # Full table\n",
    "    print(\"\\n=== All Scores ===\")\n",
    "    display(df_scores[[\"query_id\", \"category\"] + score_cols + [\"reasoning\"]].sort_values(\"category\"))\n",
    "else:\n",
    "    print(\"No valid judge scores yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Automated Evaluation - Citation Verification\n",
    "\n",
    "Checks whether cited URLs are alive and whether the source content actually supports the claims made in the research output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Extract URLs from response text\n",
    "\n",
    "def extract_urls(text: str) -> list:\n",
    "    \"\"\"Extract all URLs from text.\"\"\"\n",
    "    url_pattern = r'https?://[^\\s\\)\\]\\\"\\'>]+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    # Clean trailing punctuation\n",
    "    cleaned = []\n",
    "    for url in urls:\n",
    "        url = url.rstrip('.,;:!?')\n",
    "        if url not in cleaned:\n",
    "            cleaned.append(url)\n",
    "    return cleaned\n",
    "\n",
    "# Quick test\n",
    "test_text = \"See https://example.com/article and also https://test.org/paper.pdf for details.\"\n",
    "print(f\"Test URL extraction: {extract_urls(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Check if URLs are alive\n",
    "\n",
    "def check_url_alive(url: str, timeout: int = 10) -> dict:\n",
    "    \"\"\"Check if a URL responds with HTTP 200.\"\"\"\n",
    "    try:\n",
    "        r = requests.head(url, timeout=timeout, allow_redirects=True,\n",
    "                         headers={\"User-Agent\": \"Mozilla/5.0 (research-validator)\"})\n",
    "        return {\"url\": url, \"alive\": r.status_code < 400, \"status_code\": r.status_code}\n",
    "    except requests.exceptions.Timeout:\n",
    "        return {\"url\": url, \"alive\": False, \"status_code\": \"timeout\"}\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"alive\": False, \"status_code\": str(e)}\n",
    "\n",
    "print(\"URL liveness checker ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: LLM-based citation content verification\n",
    "\n",
    "def fetch_page_text(url: str, max_chars: int = 3000) -> Optional[str]:\n",
    "    \"\"\"Fetch and extract text content from a URL.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15, headers={\"User-Agent\": \"Mozilla/5.0 (research-validator)\"})\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # Remove script/style elements\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        return text[:max_chars]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def verify_citation_with_llm(response_text: str, url: str, source_content: str, config: dict) -> dict:\n",
    "    \"\"\"Use LLM to check if the source content supports claims near the URL in the response.\"\"\"\n",
    "    \n",
    "    # Extract text near the URL reference\n",
    "    url_pos = response_text.find(url)\n",
    "    if url_pos == -1:\n",
    "        # Try partial URL match\n",
    "        for part in url.split(\"/\")[2:4]:\n",
    "            if part in response_text:\n",
    "                url_pos = response_text.find(part)\n",
    "                break\n",
    "    \n",
    "    context_start = max(0, url_pos - 500) if url_pos >= 0 else 0\n",
    "    context_end = min(len(response_text), url_pos + 500) if url_pos >= 0 else 1000\n",
    "    claim_context = response_text[context_start:context_end]\n",
    "    \n",
    "    prompt = f\"\"\"Does the source content support the claims made in the research text near this citation?\n",
    "\n",
    "RESEARCH TEXT (near citation):\n",
    "{claim_context}\n",
    "\n",
    "SOURCE CONTENT:\n",
    "{source_content[:2000]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"verdict\": \"supported\" or \"partially_supported\" or \"unsupported\" or \"unrelated\", \"confidence\": 0.0 to 1.0, \"reasoning\": \"brief explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=300,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"verdict\": \"error\", \"confidence\": 0, \"reasoning\": f\"Unsupported provider: {provider}\"}\n",
    "        \n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        return {\"verdict\": \"error\", \"confidence\": 0, \"reasoning\": str(e)}\n",
    "\n",
    "print(\"Citation verification functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Run full citation verification pipeline\n",
    "\n",
    "CITATION_RESULTS_FILE = os.path.join(\n",
    "    CONFIG[\"output_dir\"],\n",
    "    f\"citation_verification_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\"\n",
    ")\n",
    "\n",
    "citation_results = []\n",
    "\n",
    "for result in results:\n",
    "    if not result.get(\"response_text\"):\n",
    "        continue\n",
    "    \n",
    "    urls = extract_urls(result[\"response_text\"])\n",
    "    if not urls:\n",
    "        citation_results.append({\n",
    "            \"query_id\": result[\"query_id\"],\n",
    "            \"total_citations\": 0,\n",
    "            \"alive\": 0,\n",
    "            \"dead\": 0,\n",
    "            \"supported\": 0,\n",
    "            \"unsupported\": 0,\n",
    "            \"details\": [],\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    print(f\"Verifying {len(urls)} citations for {result['query_id']}...\", end=\" \")\n",
    "    \n",
    "    query_citations = []\n",
    "    for url in urls[:10]:  # Limit to 10 URLs per query to save time/cost\n",
    "        alive_check = check_url_alive(url)\n",
    "        \n",
    "        verification = {\"verdict\": \"not_checked\"}\n",
    "        if alive_check[\"alive\"]:\n",
    "            page_text = fetch_page_text(url)\n",
    "            if page_text:\n",
    "                verification = verify_citation_with_llm(\n",
    "                    result[\"response_text\"], url, page_text, CONFIG\n",
    "                )\n",
    "        \n",
    "        query_citations.append({\n",
    "            \"url\": url,\n",
    "            \"alive\": alive_check[\"alive\"],\n",
    "            \"status_code\": alive_check[\"status_code\"],\n",
    "            \"verdict\": verification.get(\"verdict\", \"not_checked\"),\n",
    "            \"confidence\": verification.get(\"confidence\", 0),\n",
    "        })\n",
    "    \n",
    "    alive_count = sum(1 for c in query_citations if c[\"alive\"])\n",
    "    supported_count = sum(1 for c in query_citations if c[\"verdict\"] in [\"supported\", \"partially_supported\"])\n",
    "    \n",
    "    citation_results.append({\n",
    "        \"query_id\": result[\"query_id\"],\n",
    "        \"total_citations\": len(query_citations),\n",
    "        \"alive\": alive_count,\n",
    "        \"dead\": len(query_citations) - alive_count,\n",
    "        \"supported\": supported_count,\n",
    "        \"unsupported\": sum(1 for c in query_citations if c[\"verdict\"] == \"unsupported\"),\n",
    "        \"details\": query_citations,\n",
    "    })\n",
    "    \n",
    "    print(f\"alive={alive_count}/{len(query_citations)}, supported={supported_count}\")\n",
    "\n",
    "# Save\n",
    "with open(CITATION_RESULTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(citation_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nCitation results saved to: {CITATION_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Display citation verification results\n",
    "\n",
    "df_citations = pd.DataFrame([{\n",
    "    \"query_id\": c[\"query_id\"],\n",
    "    \"total_urls\": c[\"total_citations\"],\n",
    "    \"alive\": c[\"alive\"],\n",
    "    \"dead\": c[\"dead\"],\n",
    "    \"supported\": c[\"supported\"],\n",
    "    \"unsupported\": c[\"unsupported\"],\n",
    "    \"alive_rate\": c[\"alive\"] / c[\"total_citations\"] if c[\"total_citations\"] > 0 else 0,\n",
    "    \"support_rate\": c[\"supported\"] / c[\"alive\"] if c[\"alive\"] > 0 else 0,\n",
    "} for c in citation_results])\n",
    "\n",
    "if not df_citations.empty:\n",
    "    print(\"=== Citation Verification Summary ===\")\n",
    "    with_citations = df_citations[df_citations[\"total_urls\"] > 0]\n",
    "    \n",
    "    if not with_citations.empty:\n",
    "        print(f\"Queries with citations: {len(with_citations)} / {len(df_citations)}\")\n",
    "        print(f\"Average URLs per query: {with_citations['total_urls'].mean():.1f}\")\n",
    "        print(f\"Average alive rate:     {with_citations['alive_rate'].mean():.1%}\")\n",
    "        print(f\"Average support rate:   {with_citations['support_rate'].mean():.1%}\")\n",
    "        print(f\"\\nPer-query breakdown:\")\n",
    "        display(with_citations)\n",
    "    else:\n",
    "        print(\"No citations found in any responses.\")\n",
    "else:\n",
    "    print(\"No citation results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Automated Evaluation - DeepEval Metrics (Optional)\n",
    "\n",
    "Uses the DeepEval library for faithfulness and answer relevancy metrics. These metrics work **without ground truth**:\n",
    "- **FaithfulnessMetric**: Are claims grounded in the cited sources?\n",
    "- **AnswerRelevancyMetric**: Does the response address the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Run DeepEval metrics\n",
    "\n",
    "try:\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    \n",
    "    DEEPEVAL_AVAILABLE = True\n",
    "    print(\"DeepEval loaded successfully.\")\n",
    "except ImportError:\n",
    "    DEEPEVAL_AVAILABLE = False\n",
    "    print(\"DeepEval not installed. Run: pip install deepeval\")\n",
    "    print(\"Skipping this section.\")\n",
    "\n",
    "deepeval_scores = []\n",
    "\n",
    "if DEEPEVAL_AVAILABLE:\n",
    "    relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    faithfulness_metric = FaithfulnessMetric(threshold=0.5)\n",
    "    \n",
    "    for result in results:\n",
    "        if not result.get(\"response_text\"):\n",
    "            continue\n",
    "        \n",
    "        print(f\"  DeepEval scoring {result['query_id']}...\", end=\" \")\n",
    "        \n",
    "        # Extract source content as retrieval context\n",
    "        sources = result.get(\"sources\", [])\n",
    "        retrieval_context = sources if isinstance(sources, list) and sources else [result[\"response_text\"][:2000]]\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=result[\"query\"],\n",
    "            actual_output=result[\"response_text\"][:5000],\n",
    "            retrieval_context=retrieval_context[:5],\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            relevancy_metric.measure(test_case)\n",
    "            faithfulness_metric.measure(test_case)\n",
    "            \n",
    "            deepeval_scores.append({\n",
    "                \"query_id\": result[\"query_id\"],\n",
    "                \"relevancy\": relevancy_metric.score,\n",
    "                \"faithfulness\": faithfulness_metric.score,\n",
    "                \"relevancy_reason\": relevancy_metric.reason,\n",
    "                \"faithfulness_reason\": faithfulness_metric.reason,\n",
    "            })\n",
    "            print(f\"rel={relevancy_metric.score:.2f}, faith={faithfulness_metric.score:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            deepeval_scores.append({\n",
    "                \"query_id\": result[\"query_id\"],\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "    \n",
    "    # Save\n",
    "    deepeval_file = os.path.join(CONFIG[\"output_dir\"], f\"deepeval_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "    with open(deepeval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(deepeval_scores, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nDeepEval scores saved to: {deepeval_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Display DeepEval scores\n",
    "\n",
    "if deepeval_scores:\n",
    "    valid_de = [s for s in deepeval_scores if \"error\" not in s]\n",
    "    df_de = pd.DataFrame(valid_de)\n",
    "    \n",
    "    if not df_de.empty:\n",
    "        print(\"=== DeepEval Scores ===\")\n",
    "        print(f\"Average Relevancy:    {df_de['relevancy'].mean():.2f}\")\n",
    "        print(f\"Average Faithfulness: {df_de['faithfulness'].mean():.2f}\")\n",
    "        print()\n",
    "        display(df_de[[\"query_id\", \"relevancy\", \"faithfulness\"]])\n",
    "    else:\n",
    "        print(\"No valid DeepEval scores.\")\n",
    "else:\n",
    "    print(\"No DeepEval scores available. Install deepeval and re-run Section 7.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Manual Verification - Freshness Check\n",
    "\n",
    "For queries in the `manual_verification` category, you can personally check if the returned data is current. Review each response and assign a freshness score:\n",
    "- **Fresh**: Data is current (within last 7 days)\n",
    "- **Slightly Stale**: Data is 1-4 weeks old\n",
    "- **Stale**: Data is 1-6 months old\n",
    "- **Outdated**: Data is 6+ months old or incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Display manual verification queries with responses\n",
    "\n",
    "manual_results = [r for r in results if r.get(\"category\") == \"manual_verification\"]\n",
    "\n",
    "if manual_results:\n",
    "    print(f\"=== Manual Verification Queries ({len(manual_results)} queries) ===\")\n",
    "    print(\"Review each response below and assign a freshness score.\\n\")\n",
    "    \n",
    "    for r in manual_results:\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style='border: 1px solid #ccc; padding: 12px; margin: 8px 0; border-radius: 4px;'>\n",
    "            <h4>[{r['query_id']}] {r['query']}</h4>\n",
    "            <p><b>Subcategory:</b> {r.get('subcategory', 'N/A')}</p>\n",
    "            <p><b>Response (first 500 chars):</b></p>\n",
    "            <pre style='white-space: pre-wrap; background: #f5f5f5; padding: 8px;'>{r.get('response_text', 'No response')[:500]}</pre>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "else:\n",
    "    print(\"No manual verification results yet. Run Section 4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Enter manual freshness scores\n",
    "# Update the scores dict below after reviewing the responses above.\n",
    "\n",
    "# Options: \"fresh\", \"slightly_stale\", \"stale\", \"outdated\"\n",
    "MANUAL_FRESHNESS_SCORES = {\n",
    "    # \"G1a\": \"fresh\",\n",
    "    # \"G1b\": \"slightly_stale\",\n",
    "    # \"G1c\": \"stale\",\n",
    "    # \"G2a\": \"fresh\",\n",
    "    # \"G3a\": \"fresh\",\n",
    "    # \"G3b\": \"outdated\",\n",
    "    # \"G5a\": \"fresh\",\n",
    "    # \"G5b\": \"slightly_stale\",\n",
    "}\n",
    "\n",
    "# Convert to numeric for aggregation\n",
    "freshness_to_score = {\"fresh\": 4, \"slightly_stale\": 3, \"stale\": 2, \"outdated\": 1}\n",
    "\n",
    "if MANUAL_FRESHNESS_SCORES:\n",
    "    freshness_data = []\n",
    "    for qid, freshness in MANUAL_FRESHNESS_SCORES.items():\n",
    "        freshness_data.append({\n",
    "            \"query_id\": qid,\n",
    "            \"freshness\": freshness,\n",
    "            \"freshness_score\": freshness_to_score.get(freshness, 0),\n",
    "        })\n",
    "    \n",
    "    df_freshness = pd.DataFrame(freshness_data)\n",
    "    print(\"=== Manual Freshness Scores ===\")\n",
    "    print(f\"Average freshness score: {df_freshness['freshness_score'].mean():.2f} / 4.0\")\n",
    "    display(df_freshness)\n",
    "    \n",
    "    # Save\n",
    "    freshness_file = os.path.join(CONFIG[\"output_dir\"], f\"freshness_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "    with open(freshness_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(freshness_data, f, indent=2)\n",
    "    print(f\"Saved to: {freshness_file}\")\n",
    "else:\n",
    "    print(\"No freshness scores entered yet. Fill in MANUAL_FRESHNESS_SCORES above and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Results Aggregation & Visualization\n",
    "\n",
    "Combine all evaluation scores and generate visual reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Aggregate all scores into a single DataFrame\n",
    "\n",
    "# Start with basic result info\n",
    "agg_data = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        \"query_id\": r[\"query_id\"],\n",
    "        \"category\": r.get(\"category\", \"\"),\n",
    "        \"subcategory\": r.get(\"subcategory\", \"\"),\n",
    "        \"timing_seconds\": r.get(\"timing_seconds\", 0),\n",
    "        \"word_count\": r.get(\"word_count\", 0),\n",
    "        \"has_error\": bool(r.get(\"error\")),\n",
    "    }\n",
    "    \n",
    "    # Merge judge scores\n",
    "    judge = next((s for s in judge_scores if s.get(\"query_id\") == r[\"query_id\"] and \"error\" not in s), None)\n",
    "    if judge:\n",
    "        for col in [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]:\n",
    "            row[f\"judge_{col}\"] = judge.get(col)\n",
    "    \n",
    "    # Merge citation scores\n",
    "    citation = next((c for c in citation_results if c[\"query_id\"] == r[\"query_id\"]), None)\n",
    "    if citation:\n",
    "        row[\"citation_count\"] = citation[\"total_citations\"]\n",
    "        row[\"citation_alive_rate\"] = citation[\"alive\"] / citation[\"total_citations\"] if citation[\"total_citations\"] > 0 else None\n",
    "        row[\"citation_support_rate\"] = citation[\"supported\"] / citation[\"alive\"] if citation[\"alive\"] > 0 else None\n",
    "    \n",
    "    # Merge DeepEval scores\n",
    "    de = next((s for s in deepeval_scores if s.get(\"query_id\") == r[\"query_id\"] and \"error\" not in s), None)\n",
    "    if de:\n",
    "        row[\"deepeval_relevancy\"] = de.get(\"relevancy\")\n",
    "        row[\"deepeval_faithfulness\"] = de.get(\"faithfulness\")\n",
    "    \n",
    "    # Merge freshness scores\n",
    "    if r[\"query_id\"] in MANUAL_FRESHNESS_SCORES:\n",
    "        row[\"freshness\"] = MANUAL_FRESHNESS_SCORES[r[\"query_id\"]]\n",
    "        row[\"freshness_score\"] = freshness_to_score.get(MANUAL_FRESHNESS_SCORES[r[\"query_id\"]], 0)\n",
    "    \n",
    "    agg_data.append(row)\n",
    "\n",
    "df_agg = pd.DataFrame(agg_data)\n",
    "\n",
    "print(f\"=== Aggregated Results ({len(df_agg)} queries) ===\")\n",
    "display(df_agg.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32: Radar chart - overall system performance\n",
    "\n",
    "judge_cols = [c for c in df_agg.columns if c.startswith(\"judge_\")]\n",
    "\n",
    "if judge_cols:\n",
    "    avg_scores = df_agg[judge_cols].mean()\n",
    "    categories_radar = [c.replace(\"judge_\", \"\").replace(\"_\", \" \").title() for c in judge_cols]\n",
    "    values = avg_scores.values.tolist()\n",
    "    values.append(values[0])  # Close the polygon\n",
    "    categories_radar.append(categories_radar[0])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories_radar,\n",
    "        fill='toself',\n",
    "        name=CONFIG[\"system_under_test\"],\n",
    "        line_color='rgb(31, 119, 180)',\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])),\n",
    "        showlegend=True,\n",
    "        title=f\"System Performance Radar - {CONFIG['system_under_test']}\",\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No judge scores available for radar chart. Run Section 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Bar charts - scores by category\n",
    "\n",
    "if judge_cols and not df_agg.empty:\n",
    "    # Average judge score per category\n",
    "    df_agg[\"judge_avg\"] = df_agg[judge_cols].mean(axis=1)\n",
    "    \n",
    "    cat_avg = df_agg.groupby(\"category\")[\"judge_avg\"].mean().sort_values(ascending=True)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=cat_avg.values,\n",
    "        y=cat_avg.index,\n",
    "        orientation='h',\n",
    "        title=f\"Average Quality Score by Category - {CONFIG['system_under_test']}\",\n",
    "        labels={\"x\": \"Average Score (1-5)\", \"y\": \"Category\"},\n",
    "    )\n",
    "    fig.update_layout(xaxis_range=[0, 5])\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No data for bar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Latency analysis\n",
    "\n",
    "if not df_agg.empty and df_agg[\"timing_seconds\"].sum() > 0:\n",
    "    fig = px.histogram(\n",
    "        df_agg, x=\"timing_seconds\", nbins=20,\n",
    "        title=f\"Response Time Distribution - {CONFIG['system_under_test']}\",\n",
    "        labels={\"timing_seconds\": \"Response Time (seconds)\"},\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Latency Statistics:\")\n",
    "    print(f\"  Mean:   {df_agg['timing_seconds'].mean():.1f}s\")\n",
    "    print(f\"  Median: {df_agg['timing_seconds'].median():.1f}s\")\n",
    "    print(f\"  P95:    {df_agg['timing_seconds'].quantile(0.95):.1f}s\")\n",
    "    print(f\"  Max:    {df_agg['timing_seconds'].max():.1f}s\")\n",
    "    \n",
    "    # Latency by category\n",
    "    fig2 = px.box(\n",
    "        df_agg, x=\"category\", y=\"timing_seconds\",\n",
    "        title=f\"Response Time by Category - {CONFIG['system_under_test']}\",\n",
    "        labels={\"timing_seconds\": \"Response Time (seconds)\", \"category\": \"Category\"},\n",
    "    )\n",
    "    fig2.show()\n",
    "else:\n",
    "    print(\"No timing data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Save final aggregated results\n",
    "\n",
    "final_json = os.path.join(CONFIG[\"output_dir\"], f\"final_aggregated_{CONFIG['system_under_test']}_{CONFIG['run_id']}.json\")\n",
    "final_csv = os.path.join(CONFIG[\"output_dir\"], f\"final_aggregated_{CONFIG['system_under_test']}_{CONFIG['run_id']}.csv\")\n",
    "\n",
    "df_agg.to_json(final_json, orient=\"records\", indent=2)\n",
    "df_agg.to_csv(final_csv, index=False)\n",
    "\n",
    "print(f\"Final results saved:\")\n",
    "print(f\"  JSON: {final_json}\")\n",
    "print(f\"  CSV:  {final_csv}\")\n",
    "print(f\"\\nOutput directory contents:\")\n",
    "for f in sorted(os.listdir(CONFIG[\"output_dir\"])):\n",
    "    size = os.path.getsize(os.path.join(CONFIG[\"output_dir\"], f))\n",
    "    print(f\"  {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Cross-System Comparison\n",
    "\n",
    "**Run this section only after you have two validation runs to compare.**\n",
    "\n",
    "Load results from two runs (e.g., standard vs enhanced research, or different providers), then compare them head-to-head with pairwise LLM judging and visual overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 37: Load results from both systems\n",
    "\n",
    "# Update these paths to point to your saved result files from two different runs.\n",
    "SYSTEM_A_RESULTS_FILE = \"\"  # e.g. \"validation_results/results_open_deep_research_20260220_143000.json\"\n",
    "SYSTEM_B_RESULTS_FILE = \"\"  # e.g. \"validation_results/results_open_deep_research_20260221_100000.json\"\n",
    "\n",
    "# Labels for display in charts and tables\n",
    "SYSTEM_A_LABEL = \"System A\"  # e.g. \"ODR (standard)\" or \"ODR (gemini)\"\n",
    "SYSTEM_B_LABEL = \"System B\"  # e.g. \"ODR (enhanced)\" or \"ODR (openai)\"\n",
    "\n",
    "if SYSTEM_A_RESULTS_FILE and SYSTEM_B_RESULTS_FILE:\n",
    "    with open(SYSTEM_A_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        system_a_results = json.load(f)\n",
    "    with open(SYSTEM_B_RESULTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        system_b_results = json.load(f)\n",
    "    \n",
    "    print(f\"{SYSTEM_A_LABEL} results: {len(system_a_results)} queries\")\n",
    "    print(f\"{SYSTEM_B_LABEL} results: {len(system_b_results)} queries\")\n",
    "    \n",
    "    # Build lookup by query_id\n",
    "    system_a_lookup = {r[\"query_id\"]: r for r in system_a_results}\n",
    "    system_b_lookup = {r[\"query_id\"]: r for r in system_b_results}\n",
    "    common_ids = set(system_a_lookup.keys()) & set(system_b_lookup.keys())\n",
    "    print(f\"Common queries: {len(common_ids)}\")\n",
    "else:\n",
    "    print(\"Please set SYSTEM_A_RESULTS_FILE and SYSTEM_B_RESULTS_FILE paths above.\")\n",
    "    print(\"Look in your validation_results/ directory for the JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Pairwise LLM comparison\n",
    "\n",
    "def pairwise_compare(query: str, response_a: str, response_b: str, config: dict) -> dict:\n",
    "    \"\"\"Ask the judge LLM to compare two responses head-to-head.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert research evaluator. Compare two research outputs for the same query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "RESPONSE A:\n",
    "{response_a[:4000]}\n",
    "\n",
    "RESPONSE B:\n",
    "{response_b[:4000]}\n",
    "\n",
    "For each dimension, indicate which response is better (A, B, or Tie):\n",
    "1. relevancy: Which better addresses the query?\n",
    "2. depth: Which is more thorough?\n",
    "3. source_quality: Which has better citations?\n",
    "4. coherence: Which is better structured?\n",
    "5. overall: Which is the better research output overall?\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"relevancy\": \"A\" or \"B\" or \"Tie\", \"depth\": \"A\" or \"B\" or \"Tie\", \"source_quality\": \"A\" or \"B\" or \"Tie\", \"coherence\": \"A\" or \"B\" or \"Tie\", \"overall\": \"A\" or \"B\" or \"Tie\", \"reasoning\": \"brief explanation\"}}\"\"\"\n",
    "\n",
    "    provider = config.get(\"judge_provider\", \"openai\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=config.get(\"judge_model\", \"gpt-4o\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "        elif provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            resp = client.messages.create(\n",
    "                model=config.get(\"judge_model\", \"claude-sonnet-4-5-20250929\"),\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            raw = resp.content[0].text.strip()\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported provider: {provider}\"}\n",
    "        \n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Run pairwise comparison\n",
    "pairwise_results = []\n",
    "\n",
    "if 'common_ids' in dir() and common_ids:\n",
    "    for qid in sorted(common_ids):\n",
    "        a_resp = system_a_lookup[qid].get(\"response_text\", \"\")\n",
    "        b_resp = system_b_lookup[qid].get(\"response_text\", \"\")\n",
    "        \n",
    "        if not a_resp or not b_resp:\n",
    "            continue\n",
    "        \n",
    "        query_text = system_a_lookup[qid][\"query\"]\n",
    "        print(f\"Comparing {qid}...\", end=\" \")\n",
    "        \n",
    "        comparison = pairwise_compare(query_text, a_resp, b_resp, CONFIG)\n",
    "        comparison[\"query_id\"] = qid\n",
    "        pairwise_results.append(comparison)\n",
    "        \n",
    "        overall = comparison.get(\"overall\", \"?\")\n",
    "        winner = SYSTEM_A_LABEL if overall == \"A\" else SYSTEM_B_LABEL if overall == \"B\" else \"Tie\"\n",
    "        print(f\"Winner: {winner}\")\n",
    "    \n",
    "    # Save\n",
    "    pairwise_file = os.path.join(CONFIG[\"output_dir\"], f\"pairwise_comparison_{CONFIG['run_id']}.json\")\n",
    "    with open(pairwise_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pairwise_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nPairwise results saved to: {pairwise_file}\")\n",
    "else:\n",
    "    print(\"Load both system results first (Cell 37).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 39: Side-by-side comparison table\n",
    "\n",
    "if pairwise_results:\n",
    "    df_pw = pd.DataFrame(pairwise_results)\n",
    "    \n",
    "    print(\"=== Pairwise Comparison Summary ===\")\n",
    "    print(f\"A = {SYSTEM_A_LABEL}, B = {SYSTEM_B_LABEL}\\n\")\n",
    "    \n",
    "    for dim in [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\"]:\n",
    "        if dim in df_pw.columns:\n",
    "            counts = df_pw[dim].value_counts()\n",
    "            a_wins = counts.get(\"A\", 0)\n",
    "            b_wins = counts.get(\"B\", 0)\n",
    "            ties = counts.get(\"Tie\", 0)\n",
    "            print(f\"  {dim:25s}: {SYSTEM_A_LABEL}={a_wins}  {SYSTEM_B_LABEL}={b_wins}  Tie={ties}\")\n",
    "    \n",
    "    print(f\"\\nDetailed comparison:\")\n",
    "    display(df_pw[[\"query_id\", \"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\", \"reasoning\"]])\n",
    "else:\n",
    "    print(\"No pairwise results yet. Run Cell 38 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Overlay radar chart - both systems\n",
    "\n",
    "# Update these paths to point to your saved judge score files\n",
    "SYSTEM_A_JUDGE_FILE = \"\"  # e.g. \"validation_results/judge_scores_open_deep_research_20260220_143000.json\"\n",
    "SYSTEM_B_JUDGE_FILE = \"\"  # e.g. \"validation_results/judge_scores_open_deep_research_20260221_100000.json\"\n",
    "\n",
    "if SYSTEM_A_JUDGE_FILE and SYSTEM_B_JUDGE_FILE:\n",
    "    with open(SYSTEM_A_JUDGE_FILE, \"r\") as f:\n",
    "        system_a_judge = json.load(f)\n",
    "    with open(SYSTEM_B_JUDGE_FILE, \"r\") as f:\n",
    "        system_b_judge = json.load(f)\n",
    "    \n",
    "    dims = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"confidence_calibration\"]\n",
    "    dims_display = [d.replace(\"_\", \" \").title() for d in dims]\n",
    "    \n",
    "    a_valid = [s for s in system_a_judge if \"error\" not in s]\n",
    "    b_valid = [s for s in system_b_judge if \"error\" not in s]\n",
    "    \n",
    "    a_avgs = [pd.DataFrame(a_valid)[d].mean() for d in dims]\n",
    "    b_avgs = [pd.DataFrame(b_valid)[d].mean() for d in dims]\n",
    "    \n",
    "    # Close polygons\n",
    "    a_avgs.append(a_avgs[0])\n",
    "    b_avgs.append(b_avgs[0])\n",
    "    dims_display.append(dims_display[0])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(r=a_avgs, theta=dims_display, fill='toself', name=SYSTEM_A_LABEL, line_color='blue'))\n",
    "    fig.add_trace(go.Scatterpolar(r=b_avgs, theta=dims_display, fill='toself', name=SYSTEM_B_LABEL, line_color='red', opacity=0.6))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])),\n",
    "        title=f\"Head-to-Head: {SYSTEM_A_LABEL} vs {SYSTEM_B_LABEL}\",\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Set SYSTEM_A_JUDGE_FILE and SYSTEM_B_JUDGE_FILE paths to generate the overlay radar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 41: Final winner determination\n",
    "\n",
    "if pairwise_results:\n",
    "    dims = [\"relevancy\", \"depth\", \"source_quality\", \"coherence\", \"overall\"]\n",
    "    weights = {\"relevancy\": 0.2, \"depth\": 0.2, \"source_quality\": 0.2, \"coherence\": 0.15, \"overall\": 0.25}\n",
    "    \n",
    "    system_a_score = 0\n",
    "    system_b_score = 0\n",
    "    \n",
    "    for dim in dims:\n",
    "        w = weights[dim]\n",
    "        for pw in pairwise_results:\n",
    "            if pw.get(dim) == \"A\":\n",
    "                system_a_score += w\n",
    "            elif pw.get(dim) == \"B\":\n",
    "                system_b_score += w\n",
    "            else:  # Tie\n",
    "                system_a_score += w * 0.5\n",
    "                system_b_score += w * 0.5\n",
    "    \n",
    "    total = system_a_score + system_b_score\n",
    "    a_pct = system_a_score / total * 100 if total > 0 else 50\n",
    "    b_pct = system_b_score / total * 100 if total > 0 else 50\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n  {SYSTEM_A_LABEL}:  {a_pct:.1f}%\")\n",
    "    print(f\"  {SYSTEM_B_LABEL}:  {b_pct:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    if a_pct > b_pct + 5:\n",
    "        print(f\"  WINNER: {SYSTEM_A_LABEL}\")\n",
    "    elif b_pct > a_pct + 5:\n",
    "        print(f\"  WINNER: {SYSTEM_B_LABEL}\")\n",
    "    else:\n",
    "        print(f\"  RESULT: Too close to call (within 5% margin)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Run pairwise comparison first (Cell 38).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Convention check code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup - Environment & Imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Resolve project root reliably (works in VS Code + Jupyter)\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__vsc_ipynb_file__).resolve().parent\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Project root not found at {PROJECT_ROOT}. \"\n",
    "        \"Open VS Code from the project folder, or set PROJECT_ROOT manually.\"\n",
    "    )\n",
    "\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "load_dotenv(override=True)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "# Import both agents + report builder\n",
    "from extensions.agents.master_agent import MasterAgent\n",
    "from extensions.agents.data_analysis_agent import DataAnalysisAgent\n",
    "from extensions.utils.report_builder import build_html_report\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"LLM_PROVIDER: {os.getenv('LLM_PROVIDER')}\")\n",
    "print(f\"LLM_MODEL: {os.getenv('LLM_MODEL')}\")\n",
    "print(\"Environment loaded and imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Mock Research Data + Run Analysis Pipeline (~3-4 Gemini API calls)\n",
    "# We skip deep_researcher (saves ~25 API calls) and feed mock data directly\n",
    "\n",
    "QUERY = \"environment pollution 2026 and generate plots and graphs\"\n",
    "\n",
    "MOCK_RESEARCH = \"\"\"\n",
    "# Environment Pollution: Global Status Report 2026\n",
    "\n",
    "## Executive Summary\n",
    "Environmental pollution remains one of the most pressing global challenges in 2026.\n",
    "Air pollution alone causes approximately 7 million premature deaths annually according\n",
    "to the World Health Organization. Water pollution affects over 2 billion people worldwide.\n",
    "\n",
    "## Air Quality Index by Major Cities (2026)\n",
    "\n",
    "| City | Country | AQI | PM2.5 (ug/m3) | Population (millions) |\n",
    "|------|---------|-----|----------------|----------------------|\n",
    "| Delhi | India | 285 | 120.5 | 32.9 |\n",
    "| Beijing | China | 175 | 78.3 | 21.5 |\n",
    "| Lagos | Nigeria | 162 | 71.0 | 16.0 |\n",
    "| Cairo | Egypt | 155 | 68.2 | 21.3 |\n",
    "| Mumbai | India | 148 | 65.5 | 21.7 |\n",
    "| Jakarta | Indonesia | 135 | 58.9 | 10.6 |\n",
    "| Sao Paulo | Brazil | 82 | 35.4 | 12.3 |\n",
    "| London | UK | 45 | 18.2 | 9.0 |\n",
    "| New York | USA | 42 | 16.8 | 8.3 |\n",
    "| Stockholm | Sweden | 22 | 8.5 | 1.0 |\n",
    "\n",
    "## Ocean Plastic Pollution (Million Metric Tons per Year)\n",
    "\n",
    "| Year | Plastic Waste Generated | Plastic Entering Oceans | Recycling Rate Percent |\n",
    "|------|------------------------|------------------------|----------------------|\n",
    "| 2015 | 322 | 8.0 | 9.0 |\n",
    "| 2018 | 359 | 9.1 | 10.5 |\n",
    "| 2020 | 367 | 11.0 | 12.0 |\n",
    "| 2022 | 390 | 10.5 | 15.0 |\n",
    "| 2024 | 410 | 9.8 | 18.5 |\n",
    "| 2026 | 435 | 9.2 | 22.0 |\n",
    "\n",
    "## Carbon Emissions by Sector (2026, Billion Tons CO2)\n",
    "The energy sector remains the largest contributor at 15.2 billion tons CO2,\n",
    "followed by transportation at 7.8 billion tons, industry at 6.4 billion tons,\n",
    "agriculture at 5.2 billion tons, and buildings at 3.1 billion tons.\n",
    "\n",
    "## Key Findings\n",
    "1. Air pollution levels in South Asian cities remain critically high\n",
    "2. Ocean plastic pollution is stabilizing due to improved recycling rates\n",
    "3. Carbon emissions from the energy sector continue to dominate\n",
    "\"\"\"\n",
    "\n",
    "MOCK_SOURCES = [\n",
    "    \"https://www.who.int/air-pollution/2026-report\",\n",
    "    \"https://www.unep.org/ocean-plastics-2026\",\n",
    "    \"https://www.iea.org/co2-emissions-2026\",\n",
    "    \"https://www.worldbank.org/water-quality-2026\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2A: FULL PIPELINE - MasterAgent (Research + Data Analysis + Charts)\n",
    "# Uses ~25-35 API calls: web research -> data extraction -> profiling -> charts -> outliers\n",
    "# Run this OR Cell 2B (mock data), not both\n",
    "\n",
    "QUERY = \"environment pollution 2026 and if possible generate some plots and graphs\"\n",
    "\n",
    "# Initialize MasterAgent\n",
    "# use_enhanced_research=True for 3-4x deeper research (more API calls)\n",
    "agent = MasterAgent(\n",
    "    use_enhanced_research=False,  # standard mode\n",
    "    provider=os.getenv(\"LLM_PROVIDER\"),\n",
    "    model=os.getenv(\"LLM_MODEL\"),\n",
    ")\n",
    "print(f\"LLM: {type(agent.llm).__name__}\")\n",
    "print(f\"Tools: {[t.name for t in agent.tools]}\")\n",
    "\n",
    "# Run full pipeline: research -> analysis -> charts\n",
    "print(f\"\\nRunning MasterAgent with query: {QUERY}\")\n",
    "print(\"(This may take a few minutes...)\\n\")\n",
    "result = await agent.run_async(QUERY)\n",
    "\n",
    "state = result[\"state\"]\n",
    "print(f\"\\nStatus: {result['status']}\")\n",
    "print(f\"Time: {result['execution_time']:.1f}s\")\n",
    "print(f\"Agents used: {result['agents_used']}\")\n",
    "print(f\"Report length: {len(state.get('final_report', ''))} chars\")\n",
    "print(f\"Sources: {len(state.get('sources', []))}\")\n",
    "print(f\"Charts: {len(state.get('charts', []))}\")\n",
    "print(f\"Extracted data: {len(str(state.get('extracted_data', '')))} chars\")\n",
    "\n",
    "# Prepare variables for report generation (Cell 3)\n",
    "RESEARCH_TEXT = state.get(\"final_report\", \"\")\n",
    "SOURCES = state.get(\"sources\", [])\n",
    "pipeline_result = {\n",
    "    \"output\": state.get(\"analysis_output\", \"\"),\n",
    "    \"charts\": state.get(\"charts\", []),\n",
    "    \"chart_explanations\": state.get(\"chart_explanations\", {}),\n",
    "    \"extracted_data\": state.get(\"extracted_data\", \"\"),\n",
    "    \"data_profile\": state.get(\"data_profile\", \"\"),\n",
    "}\n",
    "SUB_QUERIES = state.get(\"sub_queries\", [])\n",
    "CONVERSATION_ID = result.get(\"conversation_id\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2B: LIGHTWEIGHT - DataAnalysisAgent only with mock data (~4-5 API calls)\n",
    "# Skips web research, uses mock research text directly\n",
    "# Run this OR Cell 2A (full pipeline), not both\n",
    "\n",
    "QUERY = \"environment pollution 2026 and generate plots and graphs\"\n",
    "\n",
    "# Initialize DataAnalysisAgent with Gemini\n",
    "da_agent = DataAnalysisAgent(\n",
    "    provider=os.getenv(\"LLM_PROVIDER\"),\n",
    "    model=os.getenv(\"LLM_MODEL\"),\n",
    ")\n",
    "print(f\"LLM: {type(da_agent.llm).__name__}\")\n",
    "print(f\"Structured output: extraction={da_agent.extraction_llm is not None}, explanation={da_agent.explanation_llm is not None}\")\n",
    "\n",
    "# Run pipeline on mock data\n",
    "print(\"\\nRunning analysis pipeline (~4-5 API calls)...\")\n",
    "pipeline_result = da_agent.run_pipeline(MOCK_RESEARCH)\n",
    "\n",
    "print(f\"\\nStatus: {pipeline_result['status']}\")\n",
    "print(f\"Time: {pipeline_result.get('execution_time', 0):.1f}s\")\n",
    "print(f\"Extracted data: {len(pipeline_result.get('extracted_data', ''))} chars\")\n",
    "print(f\"Data profile: {len(pipeline_result.get('data_profile', ''))} chars\")\n",
    "print(f\"Charts: {len(pipeline_result.get('charts', []))}\")\n",
    "print(f\"Chart explanations: {len(pipeline_result.get('chart_explanations', {}))}\")\n",
    "\n",
    "# Prepare variables for report generation (Cell 3)\n",
    "RESEARCH_TEXT = MOCK_RESEARCH\n",
    "SOURCES = MOCK_SOURCES\n",
    "SUB_QUERIES = []\n",
    "CONVERSATION_ID = \"mock-test-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate HTML Report (works with either Cell 2A or 2B)\n",
    "import webbrowser\n",
    "\n",
    "charts = pipeline_result.get(\"charts\", [])\n",
    "chart_explanations = pipeline_result.get(\"chart_explanations\", {})\n",
    "\n",
    "report_path = build_html_report(\n",
    "    display_text=RESEARCH_TEXT,\n",
    "    analysis_output=pipeline_result.get(\"output\", \"\"),\n",
    "    figures=charts,\n",
    "    chart_explanations=chart_explanations,\n",
    "    sources=SOURCES,\n",
    "    query=QUERY,\n",
    "    sub_queries=SUB_QUERIES,\n",
    "    conversation_id=CONVERSATION_ID,\n",
    "    src_dir=SRC_DIR,\n",
    "    extracted_data_summary=pipeline_result.get(\"extracted_data\", \"\"),\n",
    "    data_profile_summary=pipeline_result.get(\"data_profile\", \"\"),\n",
    ")\n",
    "\n",
    "print(f\"HTML report saved to: {report_path}\")\n",
    "print(f\"Report size: {os.path.getsize(report_path):,} bytes\")\n",
    "print(f\"Charts embedded: {len(charts)}\")\n",
    "\n",
    "webbrowser.open(f\"file:///{os.path.abspath(report_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Preview extracted data and chart details\n",
    "\n",
    "print(\"=== EXTRACTED DATA (first 500 chars) ===\")\n",
    "print(result.get(\"extracted_data\", \"None\")[:500])\n",
    "\n",
    "print(\"\\n=== DATA PROFILE (first 500 chars) ===\")\n",
    "print(result.get(\"data_profile\", \"None\")[:500])\n",
    "\n",
    "print(\"\\n=== CHARTS ===\")\n",
    "for path in result.get(\"charts\", []):\n",
    "    info = result.get(\"chart_explanations\", {}).get(path, {})\n",
    "    print(f\"  {os.path.basename(path)}: {info.get('title', 'N/A')}\")\n",
    "    print(f\"    {info.get('explanation', 'N/A')[:100]}\")\n",
    "\n",
    "print(f\"\\n=== PIPELINE OUTPUT ===\")\n",
    "print(result.get(\"output\", \"No output\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
